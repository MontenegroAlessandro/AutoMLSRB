{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pickle\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from Model.model_wrappers import MyMLPClassifier\n",
    "\n",
    "from Datasets.ClsDatasets import SteelPlatesFaultDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from ConfigSpace.hyperparameters import UniformFloatHyperparameter, UniformIntegerHyperparameter, \\\n",
    "    CategoricalHyperparameter, Constant\n",
    "\n",
    "from HyperparametersOptimization.hyperparemeters_optimization import TunerSMAC, TunerBOHB\n",
    "from AutomaticModelSelection.automatic_model_selection import Arm, EfficientCASHRB, AlgorithmSelectionSRB, AlgorithmSelectionAdaptiveSRB, BaseAlgorithmSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1941, 33) (1941,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AutoMLSRB/lib/python3.10/site-packages/sklearn/datasets/_openml.py:292: UserWarning: Multiple active versions of the dataset matching the name steel-plates-fault exist. Versions may be fundamentally different, returning version 1.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "data = SteelPlatesFaultDataset()\n",
    "X = data.input\n",
    "X_net = MinMaxScaler().fit_transform(data.input)\n",
    "Y = np.zeros(len(data.target), dtype=int)\n",
    "for i in range(len(data.target)):\n",
    "    if data.target[i] == '1':\n",
    "        Y[i] = 0\n",
    "    elif data.target[i] == '2':\n",
    "        Y[i] = 1\n",
    "print(X.shape, Y.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Hyperparameter(s)\n",
    "# LogisticRegression\n",
    "hp_dict_logistic_reg = dict(\n",
    "    penalty=CategoricalHyperparameter(name=\"penalty\", choices=[\"l1\", \"l2\", \"elasticnet\"], default_value=\"l2\"),\n",
    "    tol=UniformFloatHyperparameter(name=\"tol\", lower=1e-6, upper=1e-1, default_value=1e-4),\n",
    "    C=UniformFloatHyperparameter(name=\"C\", lower=0.03125, upper=10, default_value=1),\n",
    "    class_weight=CategoricalHyperparameter(name=\"class_weight\", choices=[\"balanced\"], default_value=None),\n",
    "    solver=CategoricalHyperparameter(name=\"solver\", choices=[\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"], default_value=\"lbfgs\"),\n",
    "    max_iter=UniformIntegerHyperparameter(name=\"max_iter\", lower=400, upper=1000, default_value=400)\n",
    "    #l1_ratio=UniformFloatHyperparameter(name=\"l1_ratio\", lower=0, upper=1, default_value=0.5)\n",
    ")\n",
    "\n",
    "# Support Vector Machines\n",
    "hp_dict_svm = dict(\n",
    "    C=UniformFloatHyperparameter(name=\"C\", lower=0.03125, upper=10, default_value=1),\n",
    "    kernel=CategoricalHyperparameter(name=\"kernel\", choices=[\"rbf\", \"linear\", \"poly\", \"sigmoid\"], default_value=\"rbf\"),\n",
    "    degree=UniformIntegerHyperparameter(name=\"degree\", lower=1, upper=5, default_value=3),\n",
    "    gamma=CategoricalHyperparameter(name=\"gamma\", choices=[\"scale\", \"auto\"], default_value=\"auto\"),\n",
    "    coef0=UniformFloatHyperparameter(name=\"coef0\", lower=0, upper=3, default_value=0),\n",
    "    probability=CategoricalHyperparameter(name=\"probability\", choices=[True, False], default_value=True),\n",
    "    tol=UniformFloatHyperparameter(name=\"tol\", lower=1e-6, upper=1e-1, default_value=1e-4),\n",
    "    class_weight=CategoricalHyperparameter(name=\"class_weight\", choices=[\"balanced\"], default_value=None),\n",
    "    decision_function_shape=CategoricalHyperparameter(name=\"decision_function_shape\", choices=[\"ovr\", \"ovo\"], default_value=\"ovr\")\n",
    "    # break_ties=CategoricalHyperparameter(name=\"break_ties\", choices=[True, False], default_value=True)\n",
    ")\n",
    "\n",
    "# AdaBoost\n",
    "hp_dict_adaboost = dict(\n",
    "    n_estimators=UniformIntegerHyperparameter(name=\"n_estimators\", lower=50, upper=53, default_value=50)\n",
    "    #n_estimators=UniformIntegerHyperparameter(name=\"n_estimators\", lower=50, upper=500, default_value=200),\n",
    "    #learning_rate=UniformFloatHyperparameter(name=\"learning_rate\", lower=0.01, upper=3, default_value=0.1),\n",
    "    #algorithm=CategoricalHyperparameter(name=\"algorithm\", choices=[\"SAMME.R\", \"SAMME\"], default_value=\"SAMME.R\")\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "hp_dict_xgb = dict(\n",
    "    n_estimators=UniformIntegerHyperparameter(name=\"n_estimators\", lower=50, upper=500, default_value=200),\n",
    "    eta=UniformFloatHyperparameter(name=\"eta\", lower=0.025, upper=0.3, default_value=0.3),\n",
    "    min_child_weight=UniformIntegerHyperparameter(name=\"min_child_weight\", lower=1, upper=10, default_value=1),\n",
    "    max_depth=UniformIntegerHyperparameter(name=\"max_depth\", lower=2, upper=10, default_value=6),\n",
    "    subsample=UniformFloatHyperparameter(name=\"subsample\", lower=0.5, upper=1, default_value=1),\n",
    "    gamma=UniformFloatHyperparameter(name=\"gamma\", lower=0, upper=1, default_value=0),\n",
    "    colsample_bytree=UniformFloatHyperparameter(name=\"colsample_bytree\", lower=0.5, upper=1, default_value=1.),\n",
    "    alpha=UniformFloatHyperparameter(name=\"alpha\", lower=1e-10, upper=10, default_value=1e-10),\n",
    "    lambda_t=UniformFloatHyperparameter(name=\"lambda_t\", lower=1e-10, upper=10, default_value=1e-10),\n",
    "    scale_pos_weight=CategoricalHyperparameter(name=\"scale_pos_weight\", choices=[0.01, 0.1, 1., 10, 100], default_value=1.)\n",
    ")\n",
    "\n",
    "# RandomForest\n",
    "hp_dict_rf = dict(\n",
    "    n_estimators=UniformIntegerHyperparameter(name=\"n_estimators\", lower=50, upper=500, default_value=50),\n",
    "    criterion=CategoricalHyperparameter(name=\"criterion\", choices=[\"gini\", \"entropy\", \"log_loss\"], default_value=\"gini\"),\n",
    "    max_depth=UniformIntegerHyperparameter(name=\"max_depth\", lower=1, upper=200, default_value=30),\n",
    "    min_samples_split=UniformIntegerHyperparameter(name=\"min_samples_split\", lower=2, upper=10, default_value=2),\n",
    "    min_samples_leaf=UniformIntegerHyperparameter(name=\"min_samples_leaf\", lower=2, upper=5, default_value=2),\n",
    "    max_features=CategoricalHyperparameter(name=\"max_features\", choices=[\"sqrt\", \"log2\"], default_value=\"sqrt\"),\n",
    "    bootstrap=CategoricalHyperparameter(name=\"bootstrap\", choices=[True, False], default_value=False),\n",
    "    oob_score=CategoricalHyperparameter(name=\"oob_score\", choices=[True, False], default_value=False),\n",
    "    class_weight=CategoricalHyperparameter(name=\"class_weight\", choices=[\"balanced\", \"balanced_subsample\"], default_value=None),\n",
    "    ccp_alpha=UniformFloatHyperparameter(name=\"ccp_alpha\", lower=0.0, upper=3, default_value=0.0)\n",
    ")\n",
    "\n",
    "# Extremely Randomized Trees\n",
    "hp_dict_extra_trees = deepcopy(hp_dict_rf)\n",
    "\n",
    "# KNN\n",
    "hp_dict_knn = dict(\n",
    "    n_neighbors=UniformIntegerHyperparameter(name=\"n_neighbors\", lower=1, upper=100, default_value=1),\n",
    "    weights=CategoricalHyperparameter(name=\"weights\", choices=[\"uniform\", \"distance\"], default_value=\"uniform\"),\n",
    "    algorithm=CategoricalHyperparameter(name=\"algorithm\", choices=[\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"], default_value=\"auto\"),\n",
    "    leaf_size=UniformIntegerHyperparameter(name=\"leaf_size\", lower=10, upper=50, default_value=30),\n",
    "    p=UniformIntegerHyperparameter(name=\"p\", lower=1, upper=5, default_value=2)\n",
    ")\n",
    "\n",
    "# MultiLayerPerceptron\n",
    "hp_dict_mlp = dict(\n",
    "    hidden_layer_number=UniformIntegerHyperparameter(name=\"hidden_layer_number\", lower=1, upper=5, default_value=1),\n",
    "    hidden_layer_size=UniformIntegerHyperparameter(name=\"hidden_layer_size\", lower=10, upper=100, default_value=10),\n",
    "    activation=CategoricalHyperparameter(name=\"activation\", choices=[\"identity\", \"logistic\", \"tanh\", \"relu\"], default_value=\"relu\"),\n",
    "    #solver=CategoricalHyperparameter(name=\"solver\", choices=[\"sgd\", \"adam\"], default_value=\"adam\"),\n",
    "    solver=Constant(\"solver\", \"adam\"),\n",
    "    alpha=UniformFloatHyperparameter(name=\"alpha\", lower=1e-7, upper=1., default_value=0.0001),\n",
    "    learning_rate=CategoricalHyperparameter(name=\"learning_rate\", choices=[\"adaptive\", \"invscaling\", \"constant\"], default_value=\"constant\"),\n",
    "    learning_rate_init=UniformFloatHyperparameter(name=\"learning_rate_init\", lower=1e-4, upper=3e-1, default_value=0.001),\n",
    "    tol=UniformFloatHyperparameter(name=\"tol\", lower=1e-5, upper=1e-2, default_value=1e-4),\n",
    "    momentum=UniformFloatHyperparameter(name=\"momentum\", lower=0.6, upper=1, q=0.05, default_value=0.9),\n",
    "    beta_1=UniformFloatHyperparameter(name=\"beta_1\", lower=0.6, upper=1, default_value=0.9),\n",
    "    power_t=UniformFloatHyperparameter(name=\"power_t\", lower=1e-5, upper=1, default_value=0.5),\n",
    "    max_iter=UniformIntegerHyperparameter(name=\"max_iter\", lower=200, upper=1000, default_value=200)\n",
    ")\n",
    "\n",
    "# SMBO\n",
    "hps = [hp_dict_adaboost, hp_dict_rf, hp_dict_knn, hp_dict_mlp]\n",
    "hp_dict_smbo = {}\n",
    "for elem in hps:\n",
    "    for key in elem:\n",
    "        hp_dict_smbo[key] = elem[key]\n",
    "hp_dict_smbo[\"root\"] = CategoricalHyperparameter(name=\"root\", choices=[0, 1, 2, 3], default_value=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Objective(s)\n",
    "def objective_logistic_reg(config):\n",
    "    model = LogisticRegression(\n",
    "        penalty=config[\"penalty\"],\n",
    "        tol=config[\"tol\"],\n",
    "        C=config[\"C\"],\n",
    "        class_weight=config[\"class_weight\"],\n",
    "        solver=config[\"solver\"],\n",
    "        max_iter=config[\"max_iter\"]\n",
    "        #l1_ratio=config[\"l1_ratio\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1)\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "def objective_svm(config):\n",
    "    model = SVC(\n",
    "        C=config[\"C\"],\n",
    "        kernel=config[\"kernel\"],\n",
    "        degree=config[\"degree\"],\n",
    "        gamma=config[\"gamma\"],\n",
    "        coef0=config[\"coef0\"],\n",
    "        probability=config[\"probability\"],\n",
    "        tol=config[\"tol\"],\n",
    "        class_weight=config[\"class_weight\"],\n",
    "        decision_function_shape=config[\"decision_function_shape\"]\n",
    "        #break_ties=config[\"break_ties\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1)\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "def objective_adaboost(config):\n",
    "    model = AdaBoostClassifier(\n",
    "        n_estimators=config[\"n_estimators\"]\n",
    "        #learning_rate=config[\"learning_rate\"],\n",
    "        #algorithm=config[\"algorithm\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1)\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "def objective_xgboost(config):\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=config[\"n_estimators\"],\n",
    "        eta=config[\"eta\"],\n",
    "        min_child_weight=config[\"min_child_weight\"],\n",
    "        max_depth=config[\"max_depth\"],\n",
    "        subsample=config[\"subsample\"],\n",
    "        gamma=config[\"gamma\"],\n",
    "        colsample_bytree=config[\"colsample_bytree\"],\n",
    "        alpha=config[\"alpha\"],\n",
    "        lambda_t=config[\"lambda_t\"],\n",
    "        scale_pos_weight=config[\"scale_pos_weight\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1)\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "def objective_rf(config):\n",
    "    model = RandomForestClassifier(\n",
    "        max_depth=config[\"max_depth\"],\n",
    "        criterion=config[\"criterion\"],\n",
    "        n_estimators=config[\"n_estimators\"],\n",
    "        min_samples_split=config[\"min_samples_split\"],\n",
    "        min_samples_leaf=config[\"min_samples_leaf\"],\n",
    "        max_features=config[\"max_features\"],\n",
    "        bootstrap=config[\"bootstrap\"],\n",
    "        oob_score=config[\"oob_score\"],\n",
    "        class_weight=config[\"class_weight\"],\n",
    "        ccp_alpha=config[\"ccp_alpha\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1)\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "def objective_extra_trees(config):\n",
    "    model = ExtraTreesClassifier(\n",
    "        max_depth=config[\"max_depth\"],\n",
    "        criterion=config[\"criterion\"],\n",
    "        n_estimators=config[\"n_estimators\"],\n",
    "        min_samples_split=config[\"min_samples_split\"],\n",
    "        min_samples_leaf=config[\"min_samples_leaf\"],\n",
    "        max_features=config[\"max_features\"],\n",
    "        bootstrap=config[\"bootstrap\"],\n",
    "        oob_score=config[\"oob_score\"],\n",
    "        class_weight=config[\"class_weight\"],\n",
    "        ccp_alpha=config[\"ccp_alpha\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1)\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "def objective_knn(config):\n",
    "    model = KNeighborsClassifier(\n",
    "        n_neighbors=config[\"n_neighbors\"],\n",
    "        weights=config[\"weights\"],\n",
    "        p=config[\"p\"],\n",
    "        algorithm=config[\"algorithm\"],\n",
    "        leaf_size=config[\"leaf_size\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1)\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "def objective_mlp(config):\n",
    "    my_model = MyMLPClassifier(\n",
    "        hidden_layer_size=config[\"hidden_layer_size\"],\n",
    "        hidden_layer_number=config[\"hidden_layer_number\"],\n",
    "        activation=config[\"activation\"],\n",
    "        solver=config[\"solver\"],\n",
    "        alpha=config[\"alpha\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        learning_rate_init=config[\"learning_rate_init\"],\n",
    "        power_t=config[\"power_t\"],\n",
    "        max_iter=config[\"max_iter\"],\n",
    "        # shuffle=True,\n",
    "        tol=config[\"tol\"],\n",
    "        # warm_start=True,\n",
    "        momentum=config[\"momentum\"],\n",
    "        # nesterovs_momentum=config[\"nosterovs_momentum\"],\n",
    "        beta_1=config[\"beta_1\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(my_model, X_net, Y, cv=10, n_jobs=-1)\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "def objective_smbo(config):\n",
    "    models = [\n",
    "        AdaBoostClassifier(n_estimators=config[\"n_estimators\"], learning_rate=config[\"learning_rate\"],\n",
    "                               algorithm=config[\"algorithm\"]),\n",
    "        RandomForestClassifier(max_depth=config[\"max_depth\"], criterion=config[\"criterion\"], n_estimators=config[\"n_estimators\"]),\n",
    "        KNeighborsClassifier(n_neighbors=config[\"n_neighbors\"], weights=config[\"weights\"], p=config[\"p\"]),\n",
    "        MLPClassifier(\n",
    "            activation=config[\"activation\"],\n",
    "            solver=config[\"solver\"],\n",
    "            alpha=config[\"alpha\"],\n",
    "            learning_rate=config[\"learning_rate\"],\n",
    "            learning_rate_init=config[\"learning_rate_init\"],\n",
    "            power_t=config[\"power_t\"],\n",
    "            max_iter=config[\"max_iter\"],\n",
    "            shuffle=True,\n",
    "            tol=config[\"tol\"],\n",
    "            warm_start=True,\n",
    "            momentum=config[\"momentum\"],\n",
    "            nesterovs_momentum=config[\"nosterovs_momentum\"],\n",
    "            beta_1=config[\"beta_1\"]\n",
    "        )\n",
    "    ]\n",
    "    model = models[config[\"root\"]]\n",
    "    print(\"ALGO: \", config[\"root\"])\n",
    "    scores = cross_val_score(model, X, Y, cv=10)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Tuner(s)\n",
    "base_dir = \"../experiments/Test_Exp1/\"\n",
    "tuner_args = dict(\n",
    "    hp_dict=deepcopy(hp_dict_adaboost),\n",
    "    objective_foo=objective_adaboost,\n",
    "    trials=1,\n",
    "    log_path=base_dir + \"test_ada\",\n",
    "    n_jobs=1,\n",
    "    seed=2023\n",
    ")\n",
    "tuner_adaboost = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_xgb)\n",
    "tuner_args[\"objective_foo\"] = objective_xgboost\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_xgb\"\n",
    "tuner_xgb = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_rf)\n",
    "tuner_args[\"objective_foo\"] = objective_rf\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_rf\"\n",
    "tuner_rf = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_extra_trees)\n",
    "tuner_args[\"objective_foo\"] = objective_extra_trees\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_extra_trees\"\n",
    "tuner_extra_trees = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_logistic_reg)\n",
    "tuner_args[\"objective_foo\"] = objective_logistic_reg\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_logistic\"\n",
    "tuner_logistic_reg = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_svm)\n",
    "tuner_args[\"objective_foo\"] = objective_svm\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_svm\"\n",
    "tuner_svm = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_knn)\n",
    "tuner_args[\"objective_foo\"] = objective_knn\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_knn\"\n",
    "tuner_knn = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_smbo)\n",
    "tuner_args[\"objective_foo\"] = objective_smbo\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_smbo\"\n",
    "tuner_smbo = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_mlp)\n",
    "tuner_args[\"objective_foo\"] = objective_mlp\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_mlp\"\n",
    "tuner_args[\"eta\"] = 3\n",
    "tuner_args[\"max_budget\"] = 30\n",
    "tuner_args[\"initial_budget\"] = 10\n",
    "tuner_args[\"n_jobs\"] = 1\n",
    "tuner_mlp = TunerBOHB(**tuner_args)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Arm(s)\n",
    "arm_logistic_reg = Arm(model=LogisticRegression, tuner=tuner_logistic_reg)\n",
    "arm_svm = Arm(model=SVC, tuner=tuner_svm)\n",
    "arm_adaboost = Arm(model=AdaBoostClassifier, tuner=tuner_adaboost)\n",
    "arm_xgboost = Arm(model=xgb.XGBClassifier, tuner=tuner_xgb)\n",
    "arm_rf = Arm(model=RandomForestClassifier, tuner=tuner_rf)\n",
    "arm_extra_trees = Arm(model=ExtraTreesClassifier, tuner=tuner_extra_trees)\n",
    "arm_knn = Arm(model=KNeighborsClassifier, tuner=tuner_knn)\n",
    "arm_mlp = Arm(model=MyMLPClassifier, tuner=tuner_mlp)\n",
    "\n",
    "# Dictionary of Arm(s)\n",
    "arms_dict = dict(\n",
    "    #logistic_reg=arm_logistic_reg\n",
    "    #svm=arm_svm,\n",
    "    adaboost=arm_adaboost,\n",
    "    #xgboost=arm_xgboost,\n",
    "    #random_forest=arm_rf,\n",
    "    #extra_trees=arm_extra_trees,\n",
    "    #knn=arm_knn,\n",
    "    #mlp=arm_mlp\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Log] To pull:  0\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] New best:  0  score:  0.9716494845360824\n",
      "[Log] Reward:  0.9716494845360824\n",
      "[Log] To pull:  0\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] Reward:  0.9716494845360824\n",
      "[Log] To pull:  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 10\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Base Round Robin\u001B[39;00m\n\u001B[1;32m      2\u001B[0m auto_model_generation \u001B[38;5;241m=\u001B[39m BaseAlgorithmSelection(\n\u001B[1;32m      3\u001B[0m     budget\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m,\n\u001B[1;32m      4\u001B[0m     train_data_input\u001B[38;5;241m=\u001B[39mX,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      8\u001B[0m     log_path\u001B[38;5;241m=\u001B[39mbase_dir\n\u001B[1;32m      9\u001B[0m )\n\u001B[0;32m---> 10\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mauto_model_generation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m filename \u001B[38;5;241m=\u001B[39m base_dir \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbest_model_base.sav\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     12\u001B[0m pickle\u001B[38;5;241m.\u001B[39mdump(model, \u001B[38;5;28mopen\u001B[39m(filename, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "File \u001B[0;32m~/PyCharmProjects/AutoMLSRB/AutomaticModelSelection/automatic_model_selection.py:82\u001B[0m, in \u001B[0;36mBaseAlgorithmSelection.learn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;66;03m# pull the arm\u001B[39;00m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Log] To pull: \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_pull)\n\u001B[0;32m---> 82\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marms\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlast_pull\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtuner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtune\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrials_per_step\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;66;03m# update the best\u001B[39;00m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_best()\n",
      "File \u001B[0;32m~/PyCharmProjects/AutoMLSRB/HyperparametersOptimization/hyperparemeters_optimization.py:144\u001B[0m, in \u001B[0;36mTunerSMAC.tune\u001B[0;34m(self, trials)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;66;03m# optimize\u001B[39;00m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 144\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhpoptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    146\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mincumbent\n",
      "File \u001B[0;32m/opt/anaconda3/envs/AutoMLSRB/lib/python3.10/site-packages/smac/facade/smac_ac_facade.py:723\u001B[0m, in \u001B[0;36mSMAC4AC.optimize\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    721\u001B[0m incumbent \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    722\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 723\u001B[0m     incumbent \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msolver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    724\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    725\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msolver\u001B[38;5;241m.\u001B[39msave()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/AutoMLSRB/lib/python3.10/site-packages/smac/optimizer/smbo.py:229\u001B[0m, in \u001B[0;36mSMBO.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    225\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m    227\u001B[0m \u001B[38;5;66;03m# sample next configuration for intensification\u001B[39;00m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;66;03m# Initial design runs are also included in the BO loop now.\u001B[39;00m\n\u001B[0;32m--> 229\u001B[0m intent, run_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mintensifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_next_run\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    230\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchallengers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minitial_design_configs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[43m    \u001B[49m\u001B[43mincumbent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mincumbent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchooser\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepm_chooser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    233\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrun_history\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunhistory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    234\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrepeat_configs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mintensifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrepeat_configs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    235\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtae_runner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_workers\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    236\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;66;03m# remove config from initial design challengers to not repeat it again\u001B[39;00m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minitial_design_configs \u001B[38;5;241m=\u001B[39m [c \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minitial_design_configs \u001B[38;5;28;01mif\u001B[39;00m c \u001B[38;5;241m!=\u001B[39m run_info\u001B[38;5;241m.\u001B[39mconfig]\n",
      "File \u001B[0;32m/opt/anaconda3/envs/AutoMLSRB/lib/python3.10/site-packages/smac/intensification/intensification.py:401\u001B[0m, in \u001B[0;36mIntensifier.get_next_run\u001B[0;34m(self, challengers, incumbent, chooser, run_history, repeat_configs, num_workers)\u001B[0m\n\u001B[1;32m    395\u001B[0m \u001B[38;5;66;03m# Nevertheless, if there are no more instances to run,\u001B[39;00m\n\u001B[1;32m    396\u001B[0m \u001B[38;5;66;03m# we might need to comply with line 17 and keep running the\u001B[39;00m\n\u001B[1;32m    397\u001B[0m \u001B[38;5;66;03m# same challenger. In this case, if there is not enough information\u001B[39;00m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;66;03m# to decide if the challenger is better/worst than the incumbent,\u001B[39;00m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;66;03m# line 17 doubles the number of instances to run.\u001B[39;00m\n\u001B[1;32m    400\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo further runs for challenger possible\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 401\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_racer_results\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    402\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchallenger\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchallenger\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    403\u001B[0m \u001B[43m    \u001B[49m\u001B[43mincumbent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mincumbent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    404\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrun_history\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_history\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    405\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    407\u001B[0m \u001B[38;5;66;03m# Request SMBO to skip this run. This function will\u001B[39;00m\n\u001B[1;32m    408\u001B[0m \u001B[38;5;66;03m# be called again, after the _process_racer_results\u001B[39;00m\n\u001B[1;32m    409\u001B[0m \u001B[38;5;66;03m# has updated the intensifier stage\u001B[39;00m\n\u001B[1;32m    410\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m RunInfoIntent\u001B[38;5;241m.\u001B[39mSKIP, RunInfo(\n\u001B[1;32m    411\u001B[0m     config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    412\u001B[0m     instance\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    417\u001B[0m     budget\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m,\n\u001B[1;32m    418\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/AutoMLSRB/lib/python3.10/site-packages/smac/intensification/intensification.py:813\u001B[0m, in \u001B[0;36mIntensifier._process_racer_results\u001B[0;34m(self, challenger, incumbent, run_history, log_traj)\u001B[0m\n\u001B[1;32m    809\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstage \u001B[38;5;241m=\u001B[39m IntensifierStage\u001B[38;5;241m.\u001B[39mRUN_INCUMBENT\n\u001B[1;32m    810\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontinue_challenger \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    811\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[1;32m    812\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEstimated cost of challenger on \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m runs: \u001B[39m\u001B[38;5;132;01m%.4f\u001B[39;00m\u001B[38;5;124m, but worse than incumbent\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m--> 813\u001B[0m         \u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mchal_runs\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    814\u001B[0m         chal_perf,\n\u001B[1;32m    815\u001B[0m     )\n\u001B[1;32m    817\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m new_incumbent \u001B[38;5;241m==\u001B[39m challenger:\n\u001B[1;32m    818\u001B[0m     \u001B[38;5;66;03m# New incumbent found\u001B[39;00m\n\u001B[1;32m    819\u001B[0m     incumbent \u001B[38;5;241m=\u001B[39m challenger\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Base Round Robin\n",
    "auto_model_generation = BaseAlgorithmSelection(\n",
    "    budget=5,\n",
    "    train_data_input=X,\n",
    "    train_data_output=Y,\n",
    "    arm_dictionary=arms_dict,\n",
    "    trials_per_step=2,\n",
    "    log_path=base_dir\n",
    ")\n",
    "model = auto_model_generation.learn()\n",
    "filename = base_dir + 'best_model_base.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Log] Step:  1\n",
      "[Log] Pulls:  [0. 0.]\n",
      "[Log] Scores:  [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[Log] UBs:  [1. 1.]\n",
      "[Log] LBs:  [0. 0.]\n",
      "[Log] Candidates:  {0: 0, 1: 1}\n",
      "0.7316389109172615 0.13933179504302134\n",
      "0.6645968807824479 0.017015171462225816\n",
      "[Log] New best:  0  score:  0.7316389109172615\n",
      "[Log] Reward:  0.6981178958498547\n",
      "[Log] Step:  2\n",
      "[Log] Pulls:  [1. 0.]\n",
      "[Log] Scores:  [[0.6981179 0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.        0.       ]]\n",
      "[Log] UBs:  [1. 1.]\n",
      "[Log] LBs:  [0.6981179 0.       ]\n",
      "[Log] Candidates:  {0: 0, 1: 1}\n",
      "0.6048506476341527 0.06771131719124844\n",
      "0.595593444356331 0.06577184117320085\n",
      "[Log] Reward:  0.6002220459952419\n",
      "[Log] Step:  3\n",
      "[Log] Pulls:  [1. 1.]\n",
      "[Log] Scores:  [[0.6981179  0.         0.         0.         0.        ]\n",
      " [0.60022205 0.         0.         0.         0.        ]]\n",
      "[Log] UBs:  [1. 1.]\n",
      "[Log] LBs:  [0.6981179  0.60022205]\n",
      "[Log] Candidates:  {0: 0, 1: 1}\n",
      "0.850679355009252 0.14890668945725122\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] New best:  0  score:  0.9716494845360824\n",
      "[Log] Reward:  0.9111644197726672\n",
      "[Log] Step:  4\n",
      "[Log] Pulls:  [2. 1.]\n",
      "[Log] Scores:  [[0.6981179  0.91116442 0.         0.         0.        ]\n",
      " [0.60022205 0.         0.         0.         0.        ]]\n",
      "[Log] UBs:  [1. 1.]\n",
      "[Log] LBs:  [0.91116442 0.60022205]\n",
      "[Log] Candidates:  {0: 0, 1: 1}\n",
      "0.6239280993920169 0.05928036565736087\n",
      "0.5404678826328311 0.07772883459055394\n",
      "[Log] Reward:  0.582197991012424\n",
      "[Log] Step:  5\n",
      "[Log] Pulls:  [2. 2.]\n",
      "[Log] Scores:  [[0.6981179  0.91116442 0.         0.         0.        ]\n",
      " [0.60022205 0.58219799 0.         0.         0.        ]]\n",
      "[Log] UBs:  [1.         0.56417394]\n",
      "[Log] LBs:  [0.91116442 0.58219799]\n",
      "[Log] Candidates:  {0: 0}\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] Reward:  0.9716494845360824\n"
     ]
    }
   ],
   "source": [
    "# Automatic Block Efficient CASH\n",
    "auto_model_generation = EfficientCASHRB(\n",
    "    budget=5,\n",
    "    train_data_input=X,\n",
    "    train_data_output=Y,\n",
    "    arm_dictionary=arms_dict,\n",
    "    trials_per_step=2,\n",
    "    log_path=base_dir\n",
    ")\n",
    "model = auto_model_generation.learn()\n",
    "filename = base_dir + 'best_model_ecash.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Log] Step:  0\n",
      "[Log] Pull:  0\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] New best:  0  score:  0.9716494845360824\n",
      "[Log] Reward:  0.9716494845360824\n",
      "[Log] Step:  1\n",
      "[Log] Pull:  1\n",
      "0.6244514935236585 0.06614272403314132\n",
      "0.5636241078509119 0.07381886186607216\n",
      "[Log] Reward:  0.5940378006872852\n",
      "[Log] Step:  2\n",
      "[Log] Pull:  0\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] Reward:  0.9716494845360824\n",
      "[Log] Step:  3\n",
      "[Log] Pull:  1\n",
      "0.5610362146444621 0.07052665677198691\n",
      "0.5883716627015596 0.07365354486479089\n",
      "[Log] Reward:  0.5747039386730108\n",
      "[Log] Step:  4\n",
      "[Log] Pull:  0\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] Reward:  0.9716494845360824\n"
     ]
    }
   ],
   "source": [
    "# Automatic Block SRB\n",
    "auto_model_generation = AlgorithmSelectionSRB(\n",
    "    budget=5,\n",
    "    train_data_input=X,\n",
    "    train_data_output=Y,\n",
    "    arm_dictionary=arms_dict,\n",
    "    trials_per_step=2,\n",
    "    exp_param=1,\n",
    "    eps=1/3,\n",
    "    sigma=0.1,\n",
    "    log_path=base_dir\n",
    ")\n",
    "model = auto_model_generation.learn()\n",
    "filename = base_dir + 'best_model_rucb.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Log] Step:  0\n",
      "[Log] Pull:  0\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] New best:  0  score:  0.9716494845360824\n",
      "[Log] Reward:  0.9716494845360824\n",
      "[Log] Step:  1\n",
      "[Log] Pull:  1\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.6228945281522601 0.06328409295671464\n",
      "0.6203383558022734 0.06629918719703932\n",
      "[Log] Reward:  0.6216164419772667\n",
      "[Log] Step:  2\n",
      "[Log] Pull:  0\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] Reward:  0.9716494845360824\n",
      "[Log] Step:  3\n",
      "[Log] Pull:  1\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.6167248215701824 0.06062644749745744\n",
      "0.5878614855934444 0.07147986257950514\n",
      "[Log] Reward:  0.6022931535818135\n",
      "[Log] Step:  4\n",
      "[Log] Pull:  0\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] Reward:  0.9716494845360824\n",
      "[Log] Step:  5\n",
      "[Log] Pull:  1\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.6223869944488502 0.06083890592515784\n",
      "0.6311525244514936 0.06643066019602616\n",
      "[Log] Reward:  0.6267697594501719\n",
      "[Log] Step:  6\n",
      "[Log] Pull:  0\n",
      "[Log] UBs:  [5.97164948 5.34705921]\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9711340206185568 0.08489363660222754\n",
      "[Log] Reward:  0.9713917525773196\n",
      "[Log] Step:  7\n",
      "[Log] Pull:  1\n",
      "[Log] UBs:  [4.9693299  5.34705921]\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.6285725614591594 0.05829107702296752\n",
      "0.630121596616442 0.06326284509389102\n",
      "[Log] Reward:  0.6293470790378006\n",
      "[Log] Step:  8\n",
      "[Log] Pull:  0\n",
      "[Log] UBs:  [4.9693299  4.14738832]\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] Reward:  0.9716494845360824\n",
      "[Log] Step:  9\n",
      "[Log] Pull:  1\n",
      "[Log] UBs:  [3.97319588 4.14738832]\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.6229024583663759 0.064018509986786\n",
      "0.6239360296061327 0.06503245809964552\n",
      "[Log] Reward:  0.6234192439862543\n",
      "[Log] Step:  10\n",
      "[Log] Pull:  0\n",
      "[Log] UBs:  [3.97319588 3.09378007]\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] Reward:  0.9716494845360824\n",
      "[Log] Step:  11\n",
      "[Log] Pull:  1\n",
      "[Log] UBs:  [1.85579069 3.09378007]\n",
      "[Log] Sigma:  [4.64632896e-04 5.00000000e-01]\n",
      "0.5955855141422152 0.066312872140379\n",
      "0.6223896378535554 0.058728308824667945\n",
      "[Log] Reward:  0.6089875759978853\n",
      "[Log] Step:  12\n",
      "[Log] Pull:  0\n",
      "[Log] UBs:  [1.85579069 1.30469005]\n",
      "[Log] Sigma:  [0.00046463 0.03412026]\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] Reward:  0.9716494845360824\n",
      "[Log] Step:  13\n",
      "[Log] Pull:  1\n",
      "[Log] UBs:  [0.9723356  1.30469005]\n",
      "[Log] Sigma:  [0.00037571 0.03412026]\n",
      "0.6306317737245573 0.055436785944214084\n",
      "0.5971345492994977 0.0687887525589954\n",
      "[Log] Reward:  0.6138831615120275\n",
      "[Log] Step:  14\n",
      "[Log] Pull:  0\n",
      "[Log] UBs:  [0.9723356  0.62299826]\n",
      "[Log] Sigma:  [0.00037571 0.02114453]\n",
      "0.9706238435104414 0.0847648399735056\n",
      "0.8812635474491145 0.23886592009987564\n",
      "[Log] Reward:  0.925943695479778\n"
     ]
    }
   ],
   "source": [
    "# Automatic Block Adaptive SRB\n",
    "auto_model_generation = AlgorithmSelectionAdaptiveSRB(\n",
    "    budget=15,\n",
    "    train_data_input=X,\n",
    "    train_data_output=Y,\n",
    "    arm_dictionary=arms_dict,\n",
    "    trials_per_step=2,\n",
    "    exp_param=1,\n",
    "    eps=1/3,\n",
    "    log_path=base_dir\n",
    ")\n",
    "model = auto_model_generation.learn()\n",
    "filename = base_dir + 'best_model_adarucb.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SMBO\n",
    "tuner_smbo.tune(40*10)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
