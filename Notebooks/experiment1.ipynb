{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import pickle\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from Model.model_wrappers import MyMLPClassifier\n",
    "\n",
    "from Datasets.ClsDatasets import SteelPlatesFaultDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from ConfigSpace.hyperparameters import UniformFloatHyperparameter, UniformIntegerHyperparameter, \\\n",
    "    CategoricalHyperparameter, Constant\n",
    "from ConfigSpace import EqualsCondition, InCondition\n",
    "\n",
    "from HyperparametersOptimization.hyperparemeters_optimization import TunerSMAC, TunerBOHB\n",
    "from AutomaticModelSelection.automatic_model_selection import Arm, EfficientCASHRB, AlgorithmSelectionSRB, AlgorithmSelectionAdaptiveSRB, BaseAlgorithmSelection\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1941, 33) (1941,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AutoMLSRB/lib/python3.10/site-packages/sklearn/datasets/_openml.py:292: UserWarning: Multiple active versions of the dataset matching the name steel-plates-fault exist. Versions may be fundamentally different, returning version 1.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "data = SteelPlatesFaultDataset()\n",
    "X = data.input\n",
    "X_net = MinMaxScaler().fit_transform(data.input)\n",
    "Y = np.zeros(len(data.target), dtype=int)\n",
    "for i in range(len(data.target)):\n",
    "    if data.target[i] == '1':\n",
    "        Y[i] = 0\n",
    "    elif data.target[i] == '2':\n",
    "        Y[i] = 1\n",
    "print(X.shape, Y.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Hyperparameter(s)\n",
    "# LogisticRegression\n",
    "hp_dict_logistic_reg = dict(\n",
    "    penalty=CategoricalHyperparameter(name=\"penalty\", choices=[\"l1\", \"l2\", \"elasticnet\"], default_value=\"l2\"),\n",
    "    tol=UniformFloatHyperparameter(name=\"tol\", lower=1e-6, upper=1e-1, default_value=1e-4),\n",
    "    C=UniformFloatHyperparameter(name=\"C\", lower=0.03125, upper=10, default_value=1),\n",
    "    class_weight=CategoricalHyperparameter(name=\"class_weight\", choices=[\"balanced\"], default_value=None),\n",
    "    solver=CategoricalHyperparameter(name=\"solver\", choices=[\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"], default_value=\"lbfgs\"),\n",
    "    max_iter=UniformIntegerHyperparameter(name=\"max_iter\", lower=400, upper=1000, default_value=400)\n",
    "    #l1_ratio=UniformFloatHyperparameter(name=\"l1_ratio\", lower=0, upper=1, default_value=0.5)\n",
    ")\n",
    "\n",
    "# Support Vector Machines\n",
    "hp_dict_svm = dict(\n",
    "    C=UniformFloatHyperparameter(name=\"C\", lower=0.03125, upper=1000, default_value=1),\n",
    "    #kernel=CategoricalHyperparameter(name=\"kernel\", choices=[\"rbf\", \"linear\", \"sigmoid\", \"poly\"], default_value=\"rbf\"),\n",
    "    kernel=CategoricalHyperparameter(name=\"kernel\", choices=[\"rbf\", \"sigmoid\", \"poly\"], default_value=\"rbf\"),\n",
    "    degree=UniformIntegerHyperparameter(name=\"degree\", lower=2, upper=5, default_value=2),\n",
    "    gamma=CategoricalHyperparameter(name=\"gamma\", choices=[\"scale\", \"auto\"], default_value=\"auto\"),\n",
    "    coef0=UniformFloatHyperparameter(name=\"coef0\", lower=-1, upper=1, default_value=0),\n",
    "    probability=CategoricalHyperparameter(name=\"probability\", choices=[True, False], default_value=True),\n",
    "    tol=UniformFloatHyperparameter(name=\"tol\", lower=1e-6, upper=1e-1, default_value=1e-4),\n",
    "    class_weight=CategoricalHyperparameter(name=\"class_weight\", choices=[\"balanced\"], default_value=None),\n",
    "    # decision_function_shape=CategoricalHyperparameter(name=\"decision_function_shape\", choices=[\"ovr\", \"ovo\"], default_value=\"ovr\")\n",
    "    decision_function_shape=Constant(\"decision_function_shape\", \"ovr\")\n",
    "    # break_ties=CategoricalHyperparameter(name=\"break_ties\", choices=[True, False], default_value=True)\n",
    ")\n",
    "\n",
    "# AdaBoost\n",
    "hp_dict_adaboost = dict(\n",
    "    n_estimators=UniformIntegerHyperparameter(name=\"n_estimators\", lower=50, upper=500, default_value=200),\n",
    "    learning_rate=UniformFloatHyperparameter(name=\"learning_rate\", lower=0.01, upper=3, default_value=0.1),\n",
    "    algorithm=CategoricalHyperparameter(name=\"algorithm\", choices=[\"SAMME.R\", \"SAMME\"], default_value=\"SAMME.R\")\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "hp_dict_xgb = dict(\n",
    "    n_estimators=UniformIntegerHyperparameter(name=\"n_estimators\", lower=50, upper=500, default_value=200),\n",
    "    eta=UniformFloatHyperparameter(name=\"eta\", lower=0.025, upper=0.3, default_value=0.3),\n",
    "    min_child_weight=UniformIntegerHyperparameter(name=\"min_child_weight\", lower=1, upper=10, default_value=1),\n",
    "    max_depth=UniformIntegerHyperparameter(name=\"max_depth\", lower=2, upper=10, default_value=6),\n",
    "    subsample=UniformFloatHyperparameter(name=\"subsample\", lower=0.5, upper=1, default_value=1),\n",
    "    gamma=UniformFloatHyperparameter(name=\"gamma\", lower=0, upper=1, default_value=0),\n",
    "    colsample_bytree=UniformFloatHyperparameter(name=\"colsample_bytree\", lower=0.5, upper=1, default_value=1.),\n",
    "    alpha=UniformFloatHyperparameter(name=\"alpha\", lower=1e-10, upper=10, default_value=1e-10),\n",
    "    # lambda_t=UniformFloatHyperparameter(name=\"lambda_t\", lower=1e-10, upper=10, default_value=1e-10),\n",
    "    scale_pos_weight=CategoricalHyperparameter(name=\"scale_pos_weight\", choices=[0.01, 0.1, 1., 10, 100], default_value=1.)\n",
    ")\n",
    "\n",
    "# RandomForest\n",
    "hp_dict_rf = dict(\n",
    "    n_estimators=UniformIntegerHyperparameter(name=\"n_estimators\", lower=50, upper=500, default_value=50),\n",
    "    criterion=CategoricalHyperparameter(name=\"criterion\", choices=[\"gini\", \"entropy\", \"log_loss\"], default_value=\"gini\"),\n",
    "    max_depth=UniformIntegerHyperparameter(name=\"max_depth\", lower=1, upper=200, default_value=30),\n",
    "    min_samples_split=UniformIntegerHyperparameter(name=\"min_samples_split\", lower=2, upper=10, default_value=2),\n",
    "    min_samples_leaf=UniformIntegerHyperparameter(name=\"min_samples_leaf\", lower=2, upper=5, default_value=2),\n",
    "    max_features=CategoricalHyperparameter(name=\"max_features\", choices=[\"sqrt\", \"log2\"], default_value=\"sqrt\"),\n",
    "    bootstrap=CategoricalHyperparameter(name=\"bootstrap\", choices=[True, False], default_value=False),\n",
    "    oob_score=CategoricalHyperparameter(name=\"oob_score\", choices=[True, False], default_value=False),\n",
    "    class_weight=CategoricalHyperparameter(name=\"class_weight\", choices=[\"balanced\", \"balanced_subsample\"], default_value=None),\n",
    "    ccp_alpha=UniformFloatHyperparameter(name=\"ccp_alpha\", lower=0.0, upper=3, default_value=0.0)\n",
    ")\n",
    "\n",
    "# Extremely Randomized Trees\n",
    "hp_dict_extra_trees = deepcopy(hp_dict_rf)\n",
    "\n",
    "# KNN\n",
    "hp_dict_knn = dict(\n",
    "    n_neighbors=UniformIntegerHyperparameter(name=\"n_neighbors\", lower=1, upper=100, default_value=1),\n",
    "    weights=CategoricalHyperparameter(name=\"weights\", choices=[\"uniform\", \"distance\"], default_value=\"uniform\"),\n",
    "    algorithm=CategoricalHyperparameter(name=\"algorithm\", choices=[\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"], default_value=\"auto\"),\n",
    "    leaf_size=UniformIntegerHyperparameter(name=\"leaf_size\", lower=10, upper=50, default_value=30),\n",
    "    p=UniformIntegerHyperparameter(name=\"p\", lower=1, upper=5, default_value=2)\n",
    ")\n",
    "\n",
    "# MultiLayerPerceptron\n",
    "hp_dict_mlp = dict(\n",
    "    hidden_layer_number=UniformIntegerHyperparameter(name=\"hidden_layer_number\", lower=1, upper=5, default_value=1),\n",
    "    hidden_layer_size=UniformIntegerHyperparameter(name=\"hidden_layer_size\", lower=10, upper=100, default_value=10),\n",
    "    activation=CategoricalHyperparameter(name=\"activation\", choices=[\"identity\", \"logistic\", \"tanh\", \"relu\"], default_value=\"relu\"),\n",
    "    #solver=CategoricalHyperparameter(name=\"solver\", choices=[\"sgd\", \"adam\"], default_value=\"adam\"),\n",
    "    solver=Constant(\"solver\", \"adam\"),\n",
    "    alpha=UniformFloatHyperparameter(name=\"alpha\", lower=1e-7, upper=1., default_value=0.0001),\n",
    "    learning_rate=CategoricalHyperparameter(name=\"learning_rate\", choices=[\"adaptive\", \"invscaling\", \"constant\"], default_value=\"constant\"),\n",
    "    learning_rate_init=UniformFloatHyperparameter(name=\"learning_rate_init\", lower=1e-4, upper=3e-1, default_value=0.001),\n",
    "    tol=UniformFloatHyperparameter(name=\"tol\", lower=1e-5, upper=1e-2, default_value=1e-4),\n",
    "    momentum=UniformFloatHyperparameter(name=\"momentum\", lower=0.6, upper=1, q=0.05, default_value=0.9),\n",
    "    beta_1=UniformFloatHyperparameter(name=\"beta_1\", lower=0.6, upper=1, default_value=0.9),\n",
    "    power_t=UniformFloatHyperparameter(name=\"power_t\", lower=1e-5, upper=1, default_value=0.5),\n",
    "    max_iter=UniformIntegerHyperparameter(name=\"max_iter\", lower=200, upper=1000, default_value=200)\n",
    ")\n",
    "\n",
    "# SMBO\n",
    "hps = [hp_dict_adaboost, hp_dict_rf, hp_dict_knn, hp_dict_mlp]\n",
    "hp_dict_smbo = {}\n",
    "for elem in hps:\n",
    "    for key in elem:\n",
    "        hp_dict_smbo[key] = elem[key]\n",
    "hp_dict_smbo[\"root\"] = CategoricalHyperparameter(name=\"root\", choices=[0, 1, 2, 3], default_value=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Objective(s)\n",
    "def objective_logistic_reg(config):\n",
    "    model = LogisticRegression(\n",
    "        penalty=config[\"penalty\"],\n",
    "        tol=config[\"tol\"],\n",
    "        C=config[\"C\"],\n",
    "        class_weight=config[\"class_weight\"],\n",
    "        solver=config[\"solver\"],\n",
    "        max_iter=config[\"max_iter\"]\n",
    "        #l1_ratio=config[\"l1_ratio\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1)\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "def objective_svm(config):\n",
    "    print(config)\n",
    "    model = SVC(\n",
    "        C=config[\"C\"],\n",
    "        kernel=config[\"kernel\"],\n",
    "        degree=config[\"degree\"],\n",
    "        gamma=config[\"gamma\"],\n",
    "        coef0=config[\"coef0\"],\n",
    "        probability=config[\"probability\"],\n",
    "        tol=config[\"tol\"],\n",
    "        class_weight=config[\"class_weight\"],\n",
    "        decision_function_shape=config[\"decision_function_shape\"]\n",
    "        #break_ties=config[\"break_ties\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1)\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "def objective_adaboost(config):\n",
    "    model = AdaBoostClassifier(\n",
    "        n_estimators=config[\"n_estimators\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        algorithm=config[\"algorithm\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1)\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "def objective_xgboost(config):\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=config[\"n_estimators\"],\n",
    "        eta=config[\"eta\"],\n",
    "        min_child_weight=config[\"min_child_weight\"],\n",
    "        max_depth=config[\"max_depth\"],\n",
    "        subsample=config[\"subsample\"],\n",
    "        gamma=config[\"gamma\"],\n",
    "        colsample_bytree=config[\"colsample_bytree\"],\n",
    "        alpha=config[\"alpha\"],\n",
    "        # lambda_t=config[\"lambda_t\"],\n",
    "        scale_pos_weight=config[\"scale_pos_weight\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1)\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "def objective_rf(config):\n",
    "    model = RandomForestClassifier(\n",
    "        max_depth=config[\"max_depth\"],\n",
    "        criterion=config[\"criterion\"],\n",
    "        n_estimators=config[\"n_estimators\"],\n",
    "        min_samples_split=config[\"min_samples_split\"],\n",
    "        min_samples_leaf=config[\"min_samples_leaf\"],\n",
    "        max_features=config[\"max_features\"],\n",
    "        bootstrap=config[\"bootstrap\"],\n",
    "        oob_score=config[\"oob_score\"],\n",
    "        class_weight=config[\"class_weight\"],\n",
    "        ccp_alpha=config[\"ccp_alpha\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1)\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "def objective_extra_trees(config):\n",
    "    model = ExtraTreesClassifier(\n",
    "        max_depth=config[\"max_depth\"],\n",
    "        criterion=config[\"criterion\"],\n",
    "        n_estimators=config[\"n_estimators\"],\n",
    "        min_samples_split=config[\"min_samples_split\"],\n",
    "        min_samples_leaf=config[\"min_samples_leaf\"],\n",
    "        max_features=config[\"max_features\"],\n",
    "        bootstrap=config[\"bootstrap\"],\n",
    "        oob_score=config[\"oob_score\"],\n",
    "        class_weight=config[\"class_weight\"],\n",
    "        ccp_alpha=config[\"ccp_alpha\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1)\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "def objective_knn(config):\n",
    "    model = KNeighborsClassifier(\n",
    "        n_neighbors=config[\"n_neighbors\"],\n",
    "        weights=config[\"weights\"],\n",
    "        p=config[\"p\"],\n",
    "        algorithm=config[\"algorithm\"],\n",
    "        leaf_size=config[\"leaf_size\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1)\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "def objective_mlp(config):\n",
    "    my_model = MyMLPClassifier(\n",
    "        hidden_layer_size=config[\"hidden_layer_size\"],\n",
    "        hidden_layer_number=config[\"hidden_layer_number\"],\n",
    "        activation=config[\"activation\"],\n",
    "        solver=config[\"solver\"],\n",
    "        alpha=config[\"alpha\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        learning_rate_init=config[\"learning_rate_init\"],\n",
    "        power_t=config[\"power_t\"],\n",
    "        max_iter=config[\"max_iter\"],\n",
    "        # shuffle=True,\n",
    "        tol=config[\"tol\"],\n",
    "        # warm_start=True,\n",
    "        momentum=config[\"momentum\"],\n",
    "        # nesterovs_momentum=config[\"nosterovs_momentum\"],\n",
    "        beta_1=config[\"beta_1\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(my_model, X_net, Y, cv=10, n_jobs=-1)\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "def objective_smbo(config):\n",
    "    models = [\n",
    "        AdaBoostClassifier(n_estimators=config[\"n_estimators\"], learning_rate=config[\"learning_rate\"],\n",
    "                               algorithm=config[\"algorithm\"]),\n",
    "        RandomForestClassifier(max_depth=config[\"max_depth\"], criterion=config[\"criterion\"], n_estimators=config[\"n_estimators\"]),\n",
    "        KNeighborsClassifier(n_neighbors=config[\"n_neighbors\"], weights=config[\"weights\"], p=config[\"p\"]),\n",
    "        MLPClassifier(\n",
    "            activation=config[\"activation\"],\n",
    "            solver=config[\"solver\"],\n",
    "            alpha=config[\"alpha\"],\n",
    "            learning_rate=config[\"learning_rate\"],\n",
    "            learning_rate_init=config[\"learning_rate_init\"],\n",
    "            power_t=config[\"power_t\"],\n",
    "            max_iter=config[\"max_iter\"],\n",
    "            shuffle=True,\n",
    "            tol=config[\"tol\"],\n",
    "            warm_start=True,\n",
    "            momentum=config[\"momentum\"],\n",
    "            nesterovs_momentum=config[\"nosterovs_momentum\"],\n",
    "            beta_1=config[\"beta_1\"]\n",
    "        )\n",
    "    ]\n",
    "    model = models[config[\"root\"]]\n",
    "    print(\"ALGO: \", config[\"root\"])\n",
    "    scores = cross_val_score(model, X, Y, cv=10)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Tuner(s)\n",
    "base_dir = \"../experiments/Test_Exp1/\"\n",
    "tuner_args = dict(\n",
    "    hp_dict=deepcopy(hp_dict_adaboost),\n",
    "    objective_foo=objective_adaboost,\n",
    "    trials=1,\n",
    "    log_path=base_dir + \"test_ada\",\n",
    "    n_jobs=1,\n",
    "    seed=2023,\n",
    "    conditions=None\n",
    ")\n",
    "tuner_adaboost = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_xgb)\n",
    "tuner_args[\"objective_foo\"] = objective_xgboost\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_xgb\"\n",
    "tuner_args[\"conditions\"] = None\n",
    "tuner_xgb = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_rf)\n",
    "tuner_args[\"objective_foo\"] = objective_rf\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_rf\"\n",
    "tuner_args[\"conditions\"] = None\n",
    "tuner_rf = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_extra_trees)\n",
    "tuner_args[\"objective_foo\"] = objective_extra_trees\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_extra_trees\"\n",
    "tuner_args[\"conditions\"] = None\n",
    "tuner_extra_trees = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_logistic_reg)\n",
    "tuner_args[\"objective_foo\"] = objective_logistic_reg\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_logistic\"\n",
    "tuner_args[\"conditions\"] = None\n",
    "tuner_logistic_reg = TunerSMAC(**tuner_args)\n",
    "\n",
    "conditions = [\n",
    "    EqualsCondition(hp_dict_svm[\"degree\"], hp_dict_svm[\"kernel\"], \"poly\"),\n",
    "    InCondition(hp_dict_svm[\"coef0\"], hp_dict_svm[\"kernel\"], [\"poly\", \"sigmoid\"])\n",
    "]\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_svm)\n",
    "tuner_args[\"objective_foo\"] = objective_svm\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_svm\"\n",
    "tuner_args[\"conditions\"] = conditions\n",
    "tuner_svm = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_knn)\n",
    "tuner_args[\"objective_foo\"] = objective_knn\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_knn\"\n",
    "tuner_args[\"conditions\"] = None\n",
    "tuner_knn = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_smbo)\n",
    "tuner_args[\"objective_foo\"] = objective_smbo\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_smbo\"\n",
    "tuner_args[\"conditions\"] = None\n",
    "tuner_smbo = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_mlp)\n",
    "tuner_args[\"objective_foo\"] = objective_mlp\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_mlp\"\n",
    "tuner_args[\"eta\"] = 3\n",
    "tuner_args[\"max_budget\"] = 30\n",
    "tuner_args[\"initial_budget\"] = 10\n",
    "tuner_args[\"n_jobs\"] = 1\n",
    "tuner_args[\"conditions\"] = None\n",
    "tuner_mlp = TunerBOHB(**tuner_args)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Arm(s)\n",
    "arm_logistic_reg = Arm(model=LogisticRegression, tuner=tuner_logistic_reg)\n",
    "arm_svm = Arm(model=SVC, tuner=tuner_svm)\n",
    "arm_adaboost = Arm(model=AdaBoostClassifier, tuner=tuner_adaboost)\n",
    "arm_xgboost = Arm(model=xgb.XGBClassifier, tuner=tuner_xgb)\n",
    "arm_rf = Arm(model=RandomForestClassifier, tuner=tuner_rf)\n",
    "arm_extra_trees = Arm(model=ExtraTreesClassifier, tuner=tuner_extra_trees)\n",
    "arm_knn = Arm(model=KNeighborsClassifier, tuner=tuner_knn)\n",
    "arm_mlp = Arm(model=MyMLPClassifier, tuner=tuner_mlp)\n",
    "\n",
    "# Dictionary of Arm(s)\n",
    "arms_dict = dict(\n",
    "    #logistic_reg=arm_logistic_reg\n",
    "    svm=arm_svm,\n",
    "    # adaboost=arm_adaboost,\n",
    "    #xgboost=arm_xgboost,\n",
    "    #random_forest=arm_rf,\n",
    "    #extra_trees=arm_extra_trees,\n",
    "    #knn=arm_knn,\n",
    "    #mlp=arm_mlp\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Log] To pull:  0\n",
      "Configuration(values={\n",
      "  'C': 188.39519823825685,\n",
      "  'class_weight': 'balanced',\n",
      "  'coef0': 0.04601523466408253,\n",
      "  'decision_function_shape': 'ovr',\n",
      "  'degree': 5,\n",
      "  'gamma': 'scale',\n",
      "  'kernel': 'poly',\n",
      "  'probability': True,\n",
      "  'tol': 0.02848254006407969,\n",
      "})\n",
      "\n",
      "0.5054269098598996 0.143080529838653\n",
      "Configuration(values={\n",
      "  'C': 239.574391449008,\n",
      "  'class_weight': 'balanced',\n",
      "  'coef0': 0.5725270756608654,\n",
      "  'decision_function_shape': 'ovr',\n",
      "  'gamma': 'auto',\n",
      "  'kernel': 'sigmoid',\n",
      "  'probability': False,\n",
      "  'tol': 0.009057929356372608,\n",
      "})\n",
      "\n",
      "0.0 0.0\n",
      "[Log] New best:  0  score:  0.5054269098598996\n",
      "[Log] Reward:  0.5054269098598996\n",
      "[Log] To pull:  0\n",
      "Configuration(values={\n",
      "  'C': 64.86303480986854,\n",
      "  'class_weight': 'balanced',\n",
      "  'coef0': -0.11685069195844644,\n",
      "  'decision_function_shape': 'ovr',\n",
      "  'degree': 2,\n",
      "  'gamma': 'scale',\n",
      "  'kernel': 'poly',\n",
      "  'probability': True,\n",
      "  'tol': 0.033953354979333064,\n",
      "})\n",
      "\n",
      "0.5754982817869416 0.08022992343008256\n",
      "Configuration(values={\n",
      "  'C': 1.0,\n",
      "  'class_weight': 'balanced',\n",
      "  'decision_function_shape': 'ovr',\n",
      "  'gamma': 'auto',\n",
      "  'kernel': 'rbf',\n",
      "  'probability': True,\n",
      "  'tol': 0.0001,\n",
      "})\n",
      "\n",
      "0.0 0.0\n",
      "[Log] New best:  0  score:  0.5754982817869416\n",
      "[Log] Reward:  0.5754982817869416\n",
      "[Log] To pull:  0\n",
      "Configuration(values={\n",
      "  'C': 16.78907082323841,\n",
      "  'class_weight': 'balanced',\n",
      "  'coef0': -0.3962231536441252,\n",
      "  'decision_function_shape': 'ovr',\n",
      "  'degree': 5,\n",
      "  'gamma': 'auto',\n",
      "  'kernel': 'poly',\n",
      "  'probability': True,\n",
      "  'tol': 0.09653605287703182,\n",
      "})\n",
      "\n",
      "0.0 0.0\n",
      "Configuration(values={\n",
      "  'C': 79.89017860108616,\n",
      "  'class_weight': 'balanced',\n",
      "  'coef0': 0.7082061315198072,\n",
      "  'decision_function_shape': 'ovr',\n",
      "  'gamma': 'scale',\n",
      "  'kernel': 'sigmoid',\n",
      "  'probability': False,\n",
      "  'tol': 0.08844941117971589,\n",
      "})\n",
      "\n",
      "0.0 0.0\n",
      "[Log] Reward:  0\n",
      "[Log] To pull:  0\n",
      "Configuration(values={\n",
      "  'C': 77.17694152997778,\n",
      "  'class_weight': 'balanced',\n",
      "  'coef0': -0.9969511689539976,\n",
      "  'decision_function_shape': 'ovr',\n",
      "  'gamma': 'scale',\n",
      "  'kernel': 'sigmoid',\n",
      "  'probability': False,\n",
      "  'tol': 0.09736214672772213,\n",
      "})\n",
      "\n",
      "0.0 0.0\n",
      "Configuration(values={\n",
      "  'C': 290.60096269157697,\n",
      "  'class_weight': 'balanced',\n",
      "  'coef0': -0.1506591374907993,\n",
      "  'decision_function_shape': 'ovr',\n",
      "  'degree': 4,\n",
      "  'gamma': 'scale',\n",
      "  'kernel': 'poly',\n",
      "  'probability': False,\n",
      "  'tol': 0.05160985575465708,\n",
      "})\n",
      "\n",
      "0.5873565952947395 0.09104720776265403\n",
      "[Log] New best:  0  score:  0.5873565952947395\n",
      "[Log] Reward:  0.5873565952947395\n",
      "[Log] To pull:  0\n",
      "Configuration(values={\n",
      "  'C': 396.7181550020948,\n",
      "  'class_weight': 'balanced',\n",
      "  'coef0': -0.7436012515741548,\n",
      "  'decision_function_shape': 'ovr',\n",
      "  'degree': 5,\n",
      "  'gamma': 'scale',\n",
      "  'kernel': 'poly',\n",
      "  'probability': False,\n",
      "  'tol': 0.08340653687633616,\n",
      "})\n",
      "\n",
      "0.5383848797250859 0.12566423932487183\n",
      "Configuration(values={\n",
      "  'C': 444.2320767333227,\n",
      "  'class_weight': 'balanced',\n",
      "  'coef0': -0.1891147119763601,\n",
      "  'decision_function_shape': 'ovr',\n",
      "  'degree': 3,\n",
      "  'gamma': 'scale',\n",
      "  'kernel': 'poly',\n",
      "  'probability': False,\n",
      "  'tol': 0.07367781588767605,\n",
      "})\n",
      "\n",
      "0.5646761829236056 0.11407512781758344\n",
      "[Log] Reward:  0.5515305313243457\n"
     ]
    }
   ],
   "source": [
    "# Base Round Robin\n",
    "auto_model_generation = BaseAlgorithmSelection(\n",
    "    budget=5,\n",
    "    train_data_input=X,\n",
    "    train_data_output=Y,\n",
    "    arm_dictionary=arms_dict,\n",
    "    trials_per_step=2,\n",
    "    log_path=base_dir\n",
    ")\n",
    "model = auto_model_generation.learn()\n",
    "filename = base_dir + 'best_model_base.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Log] Step:  1\n",
      "[Log] Pulls:  [0. 0.]\n",
      "[Log] Scores:  [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[Log] UBs:  [1. 1.]\n",
      "[Log] LBs:  [0. 0.]\n",
      "[Log] Candidates:  {0: 0, 1: 1}\n",
      "0.7316389109172615 0.13933179504302134\n",
      "0.6645968807824479 0.017015171462225816\n",
      "[Log] New best:  0  score:  0.7316389109172615\n",
      "[Log] Reward:  0.6981178958498547\n",
      "[Log] Step:  2\n",
      "[Log] Pulls:  [1. 0.]\n",
      "[Log] Scores:  [[0.6981179 0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.        0.       ]]\n",
      "[Log] UBs:  [1. 1.]\n",
      "[Log] LBs:  [0.6981179 0.       ]\n",
      "[Log] Candidates:  {0: 0, 1: 1}\n",
      "0.6048506476341527 0.06771131719124844\n",
      "0.595593444356331 0.06577184117320085\n",
      "[Log] Reward:  0.6002220459952419\n",
      "[Log] Step:  3\n",
      "[Log] Pulls:  [1. 1.]\n",
      "[Log] Scores:  [[0.6981179  0.         0.         0.         0.        ]\n",
      " [0.60022205 0.         0.         0.         0.        ]]\n",
      "[Log] UBs:  [1. 1.]\n",
      "[Log] LBs:  [0.6981179  0.60022205]\n",
      "[Log] Candidates:  {0: 0, 1: 1}\n",
      "0.850679355009252 0.14890668945725122\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] New best:  0  score:  0.9716494845360824\n",
      "[Log] Reward:  0.9111644197726672\n",
      "[Log] Step:  4\n",
      "[Log] Pulls:  [2. 1.]\n",
      "[Log] Scores:  [[0.6981179  0.91116442 0.         0.         0.        ]\n",
      " [0.60022205 0.         0.         0.         0.        ]]\n",
      "[Log] UBs:  [1. 1.]\n",
      "[Log] LBs:  [0.91116442 0.60022205]\n",
      "[Log] Candidates:  {0: 0, 1: 1}\n",
      "0.6239280993920169 0.05928036565736087\n",
      "0.5404678826328311 0.07772883459055394\n",
      "[Log] Reward:  0.582197991012424\n",
      "[Log] Step:  5\n",
      "[Log] Pulls:  [2. 2.]\n",
      "[Log] Scores:  [[0.6981179  0.91116442 0.         0.         0.        ]\n",
      " [0.60022205 0.58219799 0.         0.         0.        ]]\n",
      "[Log] UBs:  [1.         0.56417394]\n",
      "[Log] LBs:  [0.91116442 0.58219799]\n",
      "[Log] Candidates:  {0: 0}\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] Reward:  0.9716494845360824\n"
     ]
    }
   ],
   "source": [
    "# Automatic Block Efficient CASH\n",
    "auto_model_generation = EfficientCASHRB(\n",
    "    budget=5,\n",
    "    train_data_input=X,\n",
    "    train_data_output=Y,\n",
    "    arm_dictionary=arms_dict,\n",
    "    trials_per_step=2,\n",
    "    log_path=base_dir\n",
    ")\n",
    "model = auto_model_generation.learn()\n",
    "filename = base_dir + 'best_model_ecash.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Log] Step:  0\n",
      "[Log] Pull:  0\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] New best:  0  score:  0.9716494845360824\n",
      "[Log] Reward:  0.9716494845360824\n",
      "[Log] Step:  1\n",
      "[Log] Pull:  1\n",
      "0.6244514935236585 0.06614272403314132\n",
      "0.5636241078509119 0.07381886186607216\n",
      "[Log] Reward:  0.5940378006872852\n",
      "[Log] Step:  2\n",
      "[Log] Pull:  0\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] Reward:  0.9716494845360824\n",
      "[Log] Step:  3\n",
      "[Log] Pull:  1\n",
      "0.5610362146444621 0.07052665677198691\n",
      "0.5883716627015596 0.07365354486479089\n",
      "[Log] Reward:  0.5747039386730108\n",
      "[Log] Step:  4\n",
      "[Log] Pull:  0\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] Reward:  0.9716494845360824\n"
     ]
    }
   ],
   "source": [
    "# Automatic Block SRB\n",
    "auto_model_generation = AlgorithmSelectionSRB(\n",
    "    budget=5,\n",
    "    train_data_input=X,\n",
    "    train_data_output=Y,\n",
    "    arm_dictionary=arms_dict,\n",
    "    trials_per_step=2,\n",
    "    exp_param=1,\n",
    "    eps=1/3,\n",
    "    sigma=0.1,\n",
    "    log_path=base_dir\n",
    ")\n",
    "model = auto_model_generation.learn()\n",
    "filename = base_dir + 'best_model_rucb.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Log] Step:  0\n",
      "[Log] Pull:  0\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] New best:  0  score:  0.9716494845360824\n",
      "[Log] Reward:  0.9716494845360824\n",
      "[Log] Step:  1\n",
      "[Log] Pull:  1\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.6228945281522601 0.06328409295671464\n",
      "0.6203383558022734 0.06629918719703932\n",
      "[Log] Reward:  0.6216164419772667\n",
      "[Log] Step:  2\n",
      "[Log] Pull:  0\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] Reward:  0.9716494845360824\n",
      "[Log] Step:  3\n",
      "[Log] Pull:  1\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.6167248215701824 0.06062644749745744\n",
      "0.5878614855934444 0.07147986257950514\n",
      "[Log] Reward:  0.6022931535818135\n",
      "[Log] Step:  4\n",
      "[Log] Pull:  0\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] Reward:  0.9716494845360824\n",
      "[Log] Step:  5\n",
      "[Log] Pull:  1\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.6223869944488502 0.06083890592515784\n",
      "0.6311525244514936 0.06643066019602616\n",
      "[Log] Reward:  0.6267697594501719\n",
      "[Log] Step:  6\n",
      "[Log] Pull:  0\n",
      "[Log] UBs:  [5.97164948 5.34705921]\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9711340206185568 0.08489363660222754\n",
      "[Log] Reward:  0.9713917525773196\n",
      "[Log] Step:  7\n",
      "[Log] Pull:  1\n",
      "[Log] UBs:  [4.9693299  5.34705921]\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.6285725614591594 0.05829107702296752\n",
      "0.630121596616442 0.06326284509389102\n",
      "[Log] Reward:  0.6293470790378006\n",
      "[Log] Step:  8\n",
      "[Log] Pull:  0\n",
      "[Log] UBs:  [4.9693299  4.14738832]\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] Reward:  0.9716494845360824\n",
      "[Log] Step:  9\n",
      "[Log] Pull:  1\n",
      "[Log] UBs:  [3.97319588 4.14738832]\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.6229024583663759 0.064018509986786\n",
      "0.6239360296061327 0.06503245809964552\n",
      "[Log] Reward:  0.6234192439862543\n",
      "[Log] Step:  10\n",
      "[Log] Pull:  0\n",
      "[Log] UBs:  [3.97319588 3.09378007]\n",
      "[Log] Sigma:  [0.5 0.5]\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] Reward:  0.9716494845360824\n",
      "[Log] Step:  11\n",
      "[Log] Pull:  1\n",
      "[Log] UBs:  [1.85579069 3.09378007]\n",
      "[Log] Sigma:  [4.64632896e-04 5.00000000e-01]\n",
      "0.5955855141422152 0.066312872140379\n",
      "0.6223896378535554 0.058728308824667945\n",
      "[Log] Reward:  0.6089875759978853\n",
      "[Log] Step:  12\n",
      "[Log] Pull:  0\n",
      "[Log] UBs:  [1.85579069 1.30469005]\n",
      "[Log] Sigma:  [0.00046463 0.03412026]\n",
      "0.9716494845360824 0.08505154639175258\n",
      "0.9716494845360824 0.08505154639175258\n",
      "[Log] Reward:  0.9716494845360824\n",
      "[Log] Step:  13\n",
      "[Log] Pull:  1\n",
      "[Log] UBs:  [0.9723356  1.30469005]\n",
      "[Log] Sigma:  [0.00037571 0.03412026]\n",
      "0.6306317737245573 0.055436785944214084\n",
      "0.5971345492994977 0.0687887525589954\n",
      "[Log] Reward:  0.6138831615120275\n",
      "[Log] Step:  14\n",
      "[Log] Pull:  0\n",
      "[Log] UBs:  [0.9723356  0.62299826]\n",
      "[Log] Sigma:  [0.00037571 0.02114453]\n",
      "0.9706238435104414 0.0847648399735056\n",
      "0.8812635474491145 0.23886592009987564\n",
      "[Log] Reward:  0.925943695479778\n"
     ]
    }
   ],
   "source": [
    "# Automatic Block Adaptive SRB\n",
    "auto_model_generation = AlgorithmSelectionAdaptiveSRB(\n",
    "    budget=15,\n",
    "    train_data_input=X,\n",
    "    train_data_output=Y,\n",
    "    arm_dictionary=arms_dict,\n",
    "    trials_per_step=2,\n",
    "    exp_param=1,\n",
    "    eps=1/3,\n",
    "    log_path=base_dir\n",
    ")\n",
    "model = auto_model_generation.learn()\n",
    "filename = base_dir + 'best_model_adarucb.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SMBO\n",
    "tuner_smbo.tune(40*10)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
