{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import pickle\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from Model.model_wrappers import MyMLPClassifier\n",
    "\n",
    "from Datasets.ClsDatasets import SteelPlatesFaultDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from ConfigSpace.hyperparameters import UniformFloatHyperparameter, UniformIntegerHyperparameter, \\\n",
    "    CategoricalHyperparameter, Constant\n",
    "from ConfigSpace import EqualsCondition, InCondition\n",
    "\n",
    "from HyperparametersOptimization.hyperparemeters_optimization import TunerSMAC, TunerBOHB, TunerGenetic\n",
    "from AutomaticModelSelection.automatic_model_selection import Arm, EfficientCASHRB, AlgorithmSelectionSRB, AlgorithmSelectionAdaptiveSRB, BaseAlgorithmSelection\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1941, 33) (1941,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AutoMLSRB/lib/python3.10/site-packages/sklearn/datasets/_openml.py:292: UserWarning: Multiple active versions of the dataset matching the name steel-plates-fault exist. Versions may be fundamentally different, returning version 1.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Data keeping and preprocessing\n",
    "data = SteelPlatesFaultDataset()\n",
    "X = data.input\n",
    "Y = np.zeros(len(data.target), dtype=int)\n",
    "for i in range(len(data.target)):\n",
    "    if data.target[i] == '1':\n",
    "        Y[i] = 0\n",
    "    elif data.target[i] == '2':\n",
    "        Y[i] = 1\n",
    "print(X.shape, Y.shape)\n",
    "# data for the net need to be normalized\n",
    "X_net = MinMaxScaler().fit_transform(data.input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Hyperparameter(s)\n",
    "# LogisticRegression\n",
    "hp_dict_logistic_reg = dict(\n",
    "    penalty=Constant(\"penalty\", \"l2\"),\n",
    "    tol=UniformFloatHyperparameter(name=\"tol\", lower=1e-6, upper=1e-2, default_value=1e-4),\n",
    "    C=UniformFloatHyperparameter(name=\"C\", lower=0.01, upper=100, default_value=1),\n",
    "    class_weight=Constant(\"class_weight\", \"balanced\"),\n",
    "    solver=CategoricalHyperparameter(name=\"solver\",\n",
    "                                     choices=[\"lbfgs\", \"newton-cg\", \"sag\", \"saga\"],\n",
    "                                     default_value=\"lbfgs\"),\n",
    "    max_iter=UniformIntegerHyperparameter(name=\"max_iter\", lower=400, upper=1000, default_value=400)\n",
    ")\n",
    "\n",
    "# Support Vector Machines\n",
    "hp_dict_svm = dict(\n",
    "    C=UniformFloatHyperparameter(name=\"C\", lower=0.01, upper=100, default_value=1),\n",
    "    kernel=Constant(\"kernel\", \"rbf\"),\n",
    "    gamma=UniformFloatHyperparameter(name=\"gamma\", lower=0.005, upper=0.1, default_value=0.03),\n",
    "    tol=UniformFloatHyperparameter(name=\"tol\", lower=1e-6, upper=1e-2, default_value=1e-4),\n",
    "    class_weight=Constant(\"class_weight\", \"balanced\")\n",
    ")\n",
    "\n",
    "# AdaBoost\n",
    "hp_dict_adaboost = dict(\n",
    "    n_estimators=UniformIntegerHyperparameter(name=\"n_estimators\", lower=50, upper=500, default_value=200),\n",
    "    learning_rate=UniformFloatHyperparameter(name=\"learning_rate\", lower=0.001, upper=1, default_value=0.1),\n",
    "    algorithm=CategoricalHyperparameter(name=\"algorithm\", choices=[\"SAMME.R\", \"SAMME\"], default_value=\"SAMME.R\")\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "hp_dict_xgb = dict(\n",
    "    n_estimators=UniformIntegerHyperparameter(name=\"n_estimators\", lower=50, upper=500, default_value=200),\n",
    "    eta=UniformFloatHyperparameter(name=\"eta\", lower=0.01, upper=1, default_value=0.3),\n",
    "    min_child_weight=UniformIntegerHyperparameter(name=\"min_child_weight\", lower=1, upper=10, default_value=1),\n",
    "    max_depth=UniformIntegerHyperparameter(name=\"max_depth\", lower=4, upper=12, default_value=6),\n",
    "    subsample=UniformFloatHyperparameter(name=\"subsample\", lower=0.2, upper=1, default_value=0.5),\n",
    "    gamma=UniformFloatHyperparameter(name=\"gamma\", lower=0, upper=10, default_value=0),\n",
    "    alpha=UniformFloatHyperparameter(name=\"alpha\", lower=1e-10, upper=1, default_value=1e-10)\n",
    "    # lambda_t=UniformFloatHyperparameter(name=\"lambda_t\", lower=1e-10, upper=1, default_value=1e-10)\n",
    ")\n",
    "\n",
    "# RandomForest\n",
    "hp_dict_rf = dict(\n",
    "    n_estimators=UniformIntegerHyperparameter(name=\"n_estimators\", lower=50, upper=500, default_value=50),\n",
    "    criterion=CategoricalHyperparameter(name=\"criterion\", choices=[\"gini\", \"entropy\", \"log_loss\"],\n",
    "                                        default_value=\"gini\"),\n",
    "    max_depth=UniformIntegerHyperparameter(name=\"max_depth\", lower=4, upper=12, default_value=6),\n",
    "    max_features=CategoricalHyperparameter(name=\"max_features\", choices=[\"sqrt\", \"log2\"], default_value=\"sqrt\"),\n",
    "    bootstrap=CategoricalHyperparameter(name=\"bootstrap\", choices=[True], default_value=True),\n",
    "    oob_score=CategoricalHyperparameter(name=\"oob_score\", choices=[True], default_value=True),\n",
    "    class_weight=Constant(\"class_weight\", \"balanced\")\n",
    ")\n",
    "\n",
    "# Extremely Randomized Trees\n",
    "hp_dict_extra_trees = deepcopy(hp_dict_rf)\n",
    "\n",
    "# KNN\n",
    "hp_dict_knn = dict(\n",
    "    n_neighbors=UniformIntegerHyperparameter(name=\"n_neighbors\", lower=10, upper=100, default_value=10),\n",
    "    weights=CategoricalHyperparameter(name=\"weights\", choices=[\"uniform\", \"distance\"], default_value=\"uniform\"),\n",
    "    algorithm=CategoricalHyperparameter(name=\"algorithm\", choices=[\"ball_tree\", \"kd_tree\"],\n",
    "                                        default_value=\"kd_tree\"),\n",
    "    leaf_size=UniformIntegerHyperparameter(name=\"leaf_size\", lower=10, upper=50, default_value=30),\n",
    "    p=CategoricalHyperparameter(name=\"p\", choices=[1, 2], default_value=2)\n",
    ")\n",
    "\n",
    "# MultiLayerPerceptron\n",
    "hp_dict_mlp = dict(\n",
    "    hidden_layer_number=UniformIntegerHyperparameter(name=\"hidden_layer_number\", lower=1, upper=5, default_value=1),\n",
    "    hidden_layer_size=UniformIntegerHyperparameter(name=\"hidden_layer_size\", lower=10, upper=100, default_value=10),\n",
    "    activation=CategoricalHyperparameter(name=\"activation\", choices=[\"tanh\", \"relu\"],\n",
    "                                         default_value=\"relu\"),\n",
    "    solver=Constant(\"solver\", \"adam\"),\n",
    "    alpha=UniformFloatHyperparameter(name=\"alpha\", lower=1e-7, upper=1., default_value=0.0001),\n",
    "    learning_rate=Constant(\"learning_rate\", \"adaptive\"),\n",
    "    learning_rate_init=UniformFloatHyperparameter(name=\"learning_rate_init\", lower=1e-4, upper=1,\n",
    "                                                  default_value=0.001),\n",
    "    tol=UniformFloatHyperparameter(name=\"tol\", lower=1e-5, upper=1e-2, default_value=1e-4),\n",
    "    momentum=UniformFloatHyperparameter(name=\"momentum\", lower=0.6, upper=1, q=0.05, default_value=0.9),\n",
    "    beta_1=UniformFloatHyperparameter(name=\"beta_1\", lower=0.6, upper=1, default_value=0.9),\n",
    "    max_iter=UniformIntegerHyperparameter(name=\"max_iter\", lower=400, upper=2000, default_value=400)\n",
    ")\n",
    "\n",
    "# SMBO\n",
    "hps = [hp_dict_adaboost, hp_dict_rf, hp_dict_knn, hp_dict_mlp]\n",
    "hp_dict_smbo = {}\n",
    "for elem in hps:\n",
    "    for key in elem:\n",
    "        hp_dict_smbo[key] = elem[key]\n",
    "values = np.arange(8).tolist()\n",
    "hp_dict_smbo[\"root\"] = CategoricalHyperparameter(name=\"root\", choices=values, default_value=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Objective(s)\n",
    "def objective_logistic_reg(config):\n",
    "    model = LogisticRegression(\n",
    "        penalty=config[\"penalty\"],\n",
    "        tol=config[\"tol\"],\n",
    "        C=config[\"C\"],\n",
    "        class_weight=config[\"class_weight\"],\n",
    "        solver=config[\"solver\"],\n",
    "        max_iter=config[\"max_iter\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring=\"accuracy\")\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "\n",
    "def objective_svm(config):\n",
    "    model = SVC(\n",
    "        C=config[\"C\"],\n",
    "        kernel=config[\"kernel\"],\n",
    "        gamma=config[\"gamma\"],\n",
    "        tol=config[\"tol\"],\n",
    "        class_weight=config[\"class_weight\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring=\"accuracy\")\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "\n",
    "def objective_adaboost(config):\n",
    "    model = AdaBoostClassifier(\n",
    "        n_estimators=config[\"n_estimators\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        algorithm=config[\"algorithm\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring=\"accuracy\")\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "\n",
    "def objective_xgboost(config):\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=config[\"n_estimators\"],\n",
    "        eta=config[\"eta\"],\n",
    "        min_child_weight=config[\"min_child_weight\"],\n",
    "        max_depth=config[\"max_depth\"],\n",
    "        subsample=config[\"subsample\"],\n",
    "        gamma=config[\"gamma\"],\n",
    "        alpha=config[\"alpha\"]\n",
    "        # lambda_t=config[\"lambda_t\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring=\"accuracy\")\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "\n",
    "def objective_rf(config):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=config[\"n_estimators\"],\n",
    "        criterion=config[\"criterion\"],\n",
    "        max_depth=config[\"max_depth\"],\n",
    "        max_features=config[\"max_features\"],\n",
    "        bootstrap=config[\"bootstrap\"],\n",
    "        oob_score=config[\"oob_score\"],\n",
    "        class_weight=config[\"class_weight\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring=\"accuracy\")\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "\n",
    "def objective_extra_trees(config):\n",
    "    model = ExtraTreesClassifier(\n",
    "        n_estimators=config[\"n_estimators\"],\n",
    "        criterion=config[\"criterion\"],\n",
    "        max_depth=config[\"max_depth\"],\n",
    "        max_features=config[\"max_features\"],\n",
    "        bootstrap=config[\"bootstrap\"],\n",
    "        oob_score=config[\"oob_score\"],\n",
    "        class_weight=config[\"class_weight\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring=\"accuracy\")\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "\n",
    "def objective_knn(config):\n",
    "    model = KNeighborsClassifier(\n",
    "        n_neighbors=config[\"n_neighbors\"],\n",
    "        weights=config[\"weights\"],\n",
    "        algorithm=config[\"algorithm\"],\n",
    "        leaf_size=config[\"leaf_size\"],\n",
    "        p=config[\"p\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring=\"accuracy\")\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "\n",
    "def objective_mlp(config):\n",
    "    my_model = MyMLPClassifier(\n",
    "        hidden_layer_size=config[\"hidden_layer_size\"],\n",
    "        hidden_layer_number=config[\"hidden_layer_number\"],\n",
    "        activation=config[\"activation\"],\n",
    "        solver=config[\"solver\"],\n",
    "        alpha=config[\"alpha\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        learning_rate_init=config[\"learning_rate_init\"],\n",
    "        max_iter=config[\"max_iter\"],\n",
    "        tol=config[\"tol\"],\n",
    "        momentum=config[\"momentum\"],\n",
    "        beta_1=config[\"beta_1\"]\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_val_score(my_model, X_net, Y, cv=10, n_jobs=-1, scoring=\"accuracy\")\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()\n",
    "\n",
    "\n",
    "def objective_smbo(config):\n",
    "    models = [\n",
    "        LogisticRegression(penalty=config[\"penalty\"], tol=config[\"tol\"], C=config[\"C\"],\n",
    "                           class_weight=config[\"class_weight\"], solver=config[\"solver\"], max_iter=config[\"max_iter\"]),\n",
    "        SVC(C=config[\"C\"], kernel=config[\"kernel\"], gamma=config[\"gamma\"], tol=config[\"tol\"],\n",
    "            class_weight=config[\"class_weight\"]),\n",
    "        KNeighborsClassifier(n_neighbors=config[\"n_neighbors\"], weights=config[\"weights\"], p=config[\"p\"],\n",
    "                             algorithm=config[\"algorithm\"], leaf_size=config[\"leaf_size\"]),\n",
    "        AdaBoostClassifier(n_estimators=config[\"n_estimators\"], learning_rate=config[\"learning_rate\"],\n",
    "                           algorithm=config[\"algorithm\"]),\n",
    "        xgb.XGBClassifier(n_estimators=config[\"n_estimators\"], eta=config[\"eta\"],\n",
    "                          min_child_weight=config[\"min_child_weight\"], max_depth=config[\"max_depth\"],\n",
    "                          subsample=config[\"subsample\"], gamma=config[\"gamma\"], alpha=config[\"alpha\"]),\n",
    "        RandomForestClassifier(max_depth=config[\"max_depth\"], criterion=config[\"criterion\"],\n",
    "                               n_estimators=config[\"n_estimators\"], max_features=config[\"max_features\"],\n",
    "                               bootstrap=config[\"bootstrap\"], oob_score=config[\"oob_score\"],\n",
    "                               class_weight=config[\"class_weight\"]),\n",
    "        ExtraTreesClassifier(max_depth=config[\"max_depth\"], criterion=config[\"criterion\"],\n",
    "                             n_estimators=config[\"n_estimators\"], max_features=config[\"max_features\"],\n",
    "                             bootstrap=config[\"bootstrap\"], oob_score=config[\"oob_score\"],\n",
    "                             class_weight=config[\"class_weight\"]),\n",
    "        MyMLPClassifier(hidden_layer_size=config[\"hidden_layer_size\"],\n",
    "                        hidden_layer_number=config[\"hidden_layer_number\"],\n",
    "                        activation=config[\"activation\"], solver=config[\"solver\"],\n",
    "                        alpha=config[\"alpha\"], learning_rate=config[\"learning_rate\"],\n",
    "                        learning_rate_init=config[\"learning_rate_init\"],\n",
    "                        max_iter=config[\"max_iter\"], tol=config[\"tol\"], momentum=config[\"momentum\"],\n",
    "                        beta_1=config[\"beta_1\"])\n",
    "    ]\n",
    "    model = models[config[\"root\"]]\n",
    "    print(\"ALGO: \", config[\"root\"])\n",
    "    try:\n",
    "        if config[\"root\"] == 7:\n",
    "            scores = cross_val_score(model, X_net, Y, cv=10, n_jobs=-1, scoring=\"accuracy\")\n",
    "        else:\n",
    "            scores = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring=\"accuracy\")\n",
    "    except (ValueError, AttributeError):\n",
    "        scores = np.zeros(1)\n",
    "    print(scores.mean(), scores.std())\n",
    "    return 1 - scores.mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tuner(s)\n",
    "base_dir = \"../experiments/Tuner_Gen/\"\n",
    "tuner_args = dict(\n",
    "    hp_dict=deepcopy(hp_dict_adaboost),\n",
    "    objective_foo=objective_adaboost,\n",
    "    trials=1,\n",
    "    log_path=base_dir + \"test_ada\",\n",
    "    n_jobs=1,\n",
    "    seed=2023,\n",
    "    eta=3, initial_budget=10, max_budget=300\n",
    ")\n",
    "tuner_adaboost = TunerBOHB(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_xgb)\n",
    "tuner_args[\"objective_foo\"] = objective_xgboost\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_xgb\"\n",
    "tuner_xgb = TunerBOHB(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_rf)\n",
    "tuner_args[\"objective_foo\"] = objective_rf\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_rf\"\n",
    "tuner_rf = TunerBOHB(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_extra_trees)\n",
    "tuner_args[\"objective_foo\"] = objective_extra_trees\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_extra_trees\"\n",
    "tuner_extra_trees = TunerBOHB(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_logistic_reg)\n",
    "tuner_args[\"objective_foo\"] = objective_logistic_reg\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_logistic\"\n",
    "tuner_logistic_reg = TunerBOHB(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_svm)\n",
    "tuner_args[\"objective_foo\"] = objective_svm\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_svm\"\n",
    "tuner_svm = TunerBOHB(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_knn)\n",
    "tuner_args[\"objective_foo\"] = objective_knn\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_knn\"\n",
    "tuner_knn = TunerBOHB(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_smbo)\n",
    "tuner_args[\"objective_foo\"] = objective_smbo\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_smbo\"\n",
    "tuner_smbo = TunerBOHB(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_mlp)\n",
    "tuner_args[\"objective_foo\"] = objective_mlp\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_mlp\"\n",
    "tuner_mlp = TunerBOHB(**tuner_args)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Arm(s)\n",
    "arm_logistic_reg = Arm(model=LogisticRegression, tuner=tuner_logistic_reg)\n",
    "arm_svm = Arm(model=SVC, tuner=tuner_svm)\n",
    "arm_adaboost = Arm(model=AdaBoostClassifier, tuner=tuner_adaboost)\n",
    "arm_xgboost = Arm(model=xgb.XGBClassifier, tuner=tuner_xgb)\n",
    "arm_rf = Arm(model=RandomForestClassifier, tuner=tuner_rf)\n",
    "arm_extra_trees = Arm(model=ExtraTreesClassifier, tuner=tuner_extra_trees)\n",
    "arm_knn = Arm(model=KNeighborsClassifier, tuner=tuner_knn)\n",
    "arm_mlp = Arm(model=MyMLPClassifier, tuner=tuner_mlp)\n",
    "\n",
    "# Dictionary of Arm(s)\n",
    "arms_dict = dict(\n",
    "    # logistic_reg=arm_logistic_reg,\n",
    "    # svm=arm_svm,\n",
    "    # knn=arm_knn,\n",
    "    adaboost=arm_adaboost,\n",
    "    # xgboost=arm_xgboost,\n",
    "    # random_forest=arm_rf,\n",
    "    # extra_trees=arm_extra_trees,\n",
    "    # mlp=arm_mlp\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Debug] New Best, score: 0.9716494845360824\n",
      "[Log] Generation: 2\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.9711340206185568\n",
      "appending 1 with eval 0.9716494845360824\n",
      "appending 2 with eval 0.9706185567010308\n",
      "appending 3 with eval 0.9064842717420039\n",
      "appending 4 with eval 0.9706185567010308\n",
      "appending 5 with eval 0.9716494845360824\n",
      "appending 6 with eval 0.9716494845360824\n",
      "appending 7 with eval 0.9716494845360824\n",
      "appending 8 with eval 0.9054797779540047\n",
      "appending 9 with eval 0.9608458895056833\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "[Log] Generation: 3\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.9716494845360824\n",
      "appending 1 with eval 0.9460084588950568\n",
      "appending 2 with eval 0.9716494845360824\n",
      "appending 3 with eval 0.9039148823684906\n",
      "appending 4 with eval 0.9716494845360824\n",
      "appending 5 with eval 0.7719481892677769\n",
      "appending 6 with eval 0.9706185567010308\n",
      "appending 7 with eval 0.9716494845360824\n",
      "appending 8 with eval 0.9716494845360824\n",
      "appending 9 with eval 0.9716494845360824\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "[Log] Generation: 4\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.9716494845360824\n",
      "appending 1 with eval 0.9716494845360824\n",
      "appending 2 with eval 0.9716494845360824\n",
      "appending 3 with eval 0.9716494845360824\n",
      "appending 4 with eval 0.8863996827914354\n",
      "appending 5 with eval 0.9716494845360824\n",
      "appending 6 with eval 0.9716494845360824\n",
      "appending 7 with eval 0.9629315358181337\n",
      "appending 8 with eval 0.9716494845360824\n",
      "appending 9 with eval 0.9716494845360824\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "[Log] Generation: 5\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.9716494845360824\n",
      "appending 1 with eval 0.9716494845360824\n",
      "appending 2 with eval 0.9716494845360824\n",
      "appending 3 with eval 0.8585461274121068\n",
      "appending 4 with eval 0.882780861749934\n",
      "appending 5 with eval 0.9260031720856464\n",
      "appending 6 with eval 0.9716494845360824\n",
      "appending 7 with eval 0.9716494845360824\n",
      "appending 8 with eval 0.9716494845360824\n",
      "appending 9 with eval 0.8719376156489558\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "[Log] Generation: 6\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.9706185567010308\n",
      "appending 1 with eval 0.9716494845360824\n",
      "appending 2 with eval 0.9716494845360824\n",
      "appending 3 with eval 0.9716494845360824\n",
      "appending 4 with eval 0.9716494845360824\n",
      "appending 5 with eval 0.9716494845360824\n",
      "appending 6 with eval 0.9716494845360824\n",
      "appending 7 with eval 0.9716494845360824\n",
      "appending 8 with eval 0.9716494845360824\n",
      "appending 9 with eval 0.9716494845360824\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "[Log] Generation: 7\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.9716494845360824\n",
      "appending 1 with eval 0.9716494845360824\n",
      "appending 2 with eval 0.9716494845360824\n",
      "appending 3 with eval 0.9716494845360824\n",
      "appending 4 with eval 0.9475284166005815\n",
      "appending 5 with eval 0.9716494845360824\n",
      "appending 6 with eval 0.9172614327253502\n",
      "appending 7 with eval 0.9070103092783505\n",
      "appending 8 with eval 0.9454850647634153\n",
      "appending 9 with eval 0.9716494845360824\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "[Log] Generation: 8\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.8178139043087496\n",
      "appending 1 with eval 0.8822759714512293\n",
      "appending 2 with eval 0.9716494845360824\n",
      "appending 3 with eval 0.9716494845360824\n",
      "appending 4 with eval 0.9716494845360824\n",
      "appending 5 with eval 0.9716494845360824\n",
      "appending 6 with eval 0.9716494845360824\n",
      "appending 7 with eval 0.9716494845360824\n",
      "appending 8 with eval 0.9716494845360824\n",
      "appending 9 with eval 0.9716494845360824\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "[Log] Generation: 9\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.9716494845360824\n",
      "appending 1 with eval 0.9716494845360824\n",
      "appending 2 with eval 0.9716494845360824\n",
      "appending 3 with eval 0.9716494845360824\n",
      "appending 4 with eval 0.9716494845360824\n",
      "appending 5 with eval 0.9716494845360824\n",
      "appending 6 with eval 0.9716494845360824\n",
      "appending 7 with eval 0.9716494845360824\n",
      "appending 8 with eval 0.9716494845360824\n",
      "appending 9 with eval 0.9716494845360824\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "[Log] Generation: 10\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.8910177108115253\n",
      "appending 1 with eval 0.9706185567010308\n",
      "appending 2 with eval 0.9280518107322232\n",
      "appending 3 with eval 0.8812635474491145\n",
      "appending 4 with eval 0.902365847211208\n",
      "appending 5 with eval 0.9716494845360824\n",
      "appending 6 with eval 0.9716494845360824\n",
      "appending 7 with eval 0.9716494845360824\n",
      "appending 8 with eval 0.9716494845360824\n",
      "appending 9 with eval 0.9716494845360824\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "[Log] Generation: 11\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.9716494845360824\n",
      "appending 1 with eval 0.9280518107322232\n",
      "appending 2 with eval 0.9716494845360824\n",
      "appending 3 with eval 0.9716494845360824\n",
      "appending 4 with eval 0.9716494845360824\n",
      "appending 5 with eval 0.9716494845360824\n",
      "appending 6 with eval 0.9716494845360824\n",
      "appending 7 with eval 0.9716494845360824\n",
      "appending 8 with eval 0.9716494845360824\n",
      "appending 9 with eval 0.9716494845360824\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "[Log] Generation: 12\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.9716494845360824\n",
      "appending 1 with eval 0.860081945545863\n",
      "appending 2 with eval 0.9716494845360824\n",
      "appending 3 with eval 0.9716494845360824\n",
      "appending 4 with eval 0.9701030927835053\n",
      "appending 5 with eval 0.9716494845360824\n",
      "appending 6 with eval 0.9716494845360824\n",
      "appending 7 with eval 0.9023896378535554\n",
      "appending 8 with eval 0.9716494845360824\n",
      "appending 9 with eval 0.9716494845360824\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "[Log] Generation: 13\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.9716494845360824\n",
      "appending 1 with eval 0.9716494845360824\n",
      "appending 2 with eval 0.9716494845360824\n",
      "appending 3 with eval 0.9023896378535554\n",
      "appending 4 with eval 0.867837694951097\n",
      "appending 5 with eval 0.9716494845360824\n",
      "appending 6 with eval 0.9654956383822363\n",
      "appending 7 with eval 0.9716494845360824\n",
      "appending 8 with eval 0.9716494845360824\n",
      "appending 9 with eval 0.9716494845360824\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "[Log] Generation: 14\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.9716494845360824\n",
      "appending 1 with eval 0.9716494845360824\n",
      "appending 2 with eval 0.8889294210943696\n",
      "appending 3 with eval 0.9716494845360824\n",
      "appending 4 with eval 0.9716494845360824\n",
      "appending 5 with eval 0.8884615384615385\n",
      "appending 6 with eval 0.8801771081152525\n",
      "appending 7 with eval 0.9716494845360824\n",
      "appending 8 with eval 0.9716494845360824\n",
      "appending 9 with eval 0.8920618556701031\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "[Log] Generation: 15\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.9716494845360824\n",
      "appending 1 with eval 0.9659820248480042\n",
      "appending 2 with eval 0.9716494845360824\n",
      "appending 3 with eval 0.9716494845360824\n",
      "appending 4 with eval 0.9716494845360824\n",
      "appending 5 with eval 0.9716494845360824\n",
      "appending 6 with eval 0.9716494845360824\n",
      "appending 7 with eval 0.9716494845360824\n",
      "appending 8 with eval 0.9716494845360824\n",
      "appending 9 with eval 0.9716494845360824\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "[Log] Generation: 16\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.9716494845360824\n",
      "appending 1 with eval 0.8838012159661645\n",
      "appending 2 with eval 0.9716494845360824\n",
      "appending 3 with eval 0.9716494845360824\n",
      "appending 4 with eval 0.9716494845360824\n",
      "appending 5 with eval 0.9716494845360824\n",
      "appending 6 with eval 0.9675257731958762\n",
      "appending 7 with eval 0.9716494845360824\n",
      "appending 8 with eval 0.7993074279672218\n",
      "appending 9 with eval 0.9716494845360824\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "[Log] Generation: 17\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.9716494845360824\n",
      "appending 1 with eval 0.8920644990748083\n",
      "appending 2 with eval 0.9716494845360824\n",
      "appending 3 with eval 0.9716494845360824\n",
      "appending 4 with eval 0.9716494845360824\n",
      "appending 5 with eval 0.9716494845360824\n",
      "appending 6 with eval 0.9680412371134022\n",
      "appending 7 with eval 0.8214274385408405\n",
      "appending 8 with eval 0.9716494845360824\n",
      "appending 9 with eval 0.9716494845360824\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "[Log] Generation: 18\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.9716494845360824\n",
      "appending 1 with eval 0.9716494845360824\n",
      "appending 2 with eval 0.9716494845360824\n",
      "appending 3 with eval 0.879714512291832\n",
      "appending 4 with eval 0.9716494845360824\n",
      "appending 5 with eval 0.9716494845360824\n",
      "appending 6 with eval 0.9106159132963256\n",
      "appending 7 with eval 0.9716494845360824\n",
      "appending 8 with eval 0.9716494845360824\n",
      "appending 9 with eval 0.9716494845360824\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "[Log] Generation: 19\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.9588236849061591\n",
      "appending 1 with eval 0.9716494845360824\n",
      "appending 2 with eval 0.9716494845360824\n",
      "appending 3 with eval 0.9008485329103886\n",
      "appending 4 with eval 0.9716494845360824\n",
      "appending 5 with eval 0.9716494845360824\n",
      "appending 6 with eval 0.9716494845360824\n",
      "appending 7 with eval 0.9716494845360824\n",
      "appending 8 with eval 0.9716494845360824\n",
      "appending 9 with eval 0.9059793814432989\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "[Log] Generation: 20\n",
      "Previous generation best agent evaluation: 0.9716494845360824\n",
      "best appending\n",
      "appending 0 with eval 0.9716494845360824\n",
      "appending 1 with eval 0.9716494845360824\n",
      "appending 2 with eval 0.8936056040179752\n",
      "appending 3 with eval 0.9716494845360824\n",
      "appending 4 with eval 0.9716494845360824\n",
      "appending 5 with eval 0.9716494845360824\n",
      "appending 6 with eval 0.9716494845360824\n",
      "appending 7 with eval 0.9383161512027491\n",
      "appending 8 with eval 0.9675257731958762\n",
      "appending 9 with eval 0.9018768173407349\n",
      "[Debug] Not new best\n",
      "Generation End\n",
      "Best ag: {'config': {'alpha': 0.2034614919527147, 'eta': 0.32739841308891476, 'gamma': 3.7656378276267812, 'max_depth': 5, 'min_child_weight': 2, 'n_estimators': 255, 'subsample': 0.3566910699931598}, 'block_eval': 0.9716494845360824}\n",
      "Best res: 0.9716494845360824\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'config': {'alpha': 0.2034614919527147,\n  'eta': 0.32739841308891476,\n  'gamma': 3.7656378276267812,\n  'max_depth': 5,\n  'min_child_weight': 2,\n  'n_estimators': 255,\n  'subsample': 0.3566910699931598},\n 'block_eval': 0.9716494845360824}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner_gen_xgb = TunerGenetic(\n",
    "    n_agents=10,\n",
    "    n_generations=20,\n",
    "    prob_point_mutation=0.5,\n",
    "    tuning_mode=\"best_performant_elitism\",\n",
    "    pool_size=5,\n",
    "    objective=objective_xgboost,\n",
    "    hp_dict=deepcopy(hp_dict_xgb),\n",
    "    seed=2023,\n",
    "    n_jobs=-1,\n",
    "    which_res=\"cost\",\n",
    "    log_path=\"../experiments/test_genetic/test_xgb\"\n",
    ")\n",
    "tuner_gen_xgb.tune()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 1\n",
      "[Debug] New Best, score: 0.9716494845360824\n",
      "Generation: 2\n",
      "[Debug] Not new best\n",
      "Generation: 3\n",
      "[Debug] Not new best\n",
      "Generation: 4\n",
      "[Debug] Not new best\n",
      "Generation: 5\n",
      "[Debug] Not new best\n",
      "Generation: 6\n",
      "[Debug] Not new best\n",
      "Generation: 7\n",
      "[Debug] Not new best\n",
      "Generation: 8\n",
      "[Debug] Not new best\n",
      "Generation: 9\n",
      "[Debug] Not new best\n",
      "Generation: 10\n",
      "[Debug] Not new best\n",
      "Generation: 11\n",
      "[Debug] Not new best\n",
      "Generation: 12\n",
      "[Debug] Not new best\n",
      "Generation: 13\n",
      "[Debug] Not new best\n",
      "Generation: 14\n",
      "[Debug] Not new best\n",
      "Generation: 15\n",
      "[Debug] Not new best\n",
      "Generation: 16\n",
      "[Debug] Not new best\n",
      "Generation: 17\n",
      "[Debug] Not new best\n",
      "Generation: 18\n",
      "[Debug] Not new best\n",
      "Generation: 19\n",
      "[Debug] Not new best\n",
      "Best ag: {'config': {'alpha': 0.2034614919527147, 'eta': 0.32739841308891476, 'gamma': 3.7656378276267812, 'max_depth': 5, 'min_child_weight': 2, 'n_estimators': 255, 'subsample': 0.3566910699931598}, 'block_eval': 0.9716494845360824}\n",
      "Best res: 0.9716494845360824\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'config': {'alpha': 0.2034614919527147,\n  'eta': 0.32739841308891476,\n  'gamma': 3.7656378276267812,\n  'max_depth': 5,\n  'min_child_weight': 2,\n  'n_estimators': 255,\n  'subsample': 0.3566910699931598},\n 'block_eval': 0.9716494845360824}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner_gen_xgb = TunerGenetic(\n",
    "    n_agents=10,\n",
    "    n_generations=20,\n",
    "    prob_point_mutation=0.5,\n",
    "    tuning_mode=\"pool_elitism\",\n",
    "    pool_size=2,\n",
    "    objective=objective_xgboost,\n",
    "    hp_dict=deepcopy(hp_dict_xgb),\n",
    "    seed=2023,\n",
    "    n_jobs=-1,\n",
    "    which_res=\"cost\",\n",
    "    log_path=\"../experiments/test_genetic_pool\"\n",
    ")\n",
    "tuner_gen_xgb.tune()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "budget = 50\n",
    "for arm in arms_dict:\n",
    "    print(\"Serving: \" + str(arm))\n",
    "    arms_dict[arm].tuner.tune(budget)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tuner(s)\n",
    "base_dir = \"experiments/Tuner_Curves_SMAC/\"\n",
    "tuner_args = dict(\n",
    "    hp_dict=deepcopy(hp_dict_adaboost),\n",
    "    objective_foo=objective_adaboost,\n",
    "    trials=1,\n",
    "    log_path=base_dir + \"test_ada\",\n",
    "    n_jobs=1,\n",
    "    seed=2023\n",
    ")\n",
    "tuner_adaboost = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_xgb)\n",
    "tuner_args[\"objective_foo\"] = objective_xgboost\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_xgb\"\n",
    "tuner_xgb = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_rf)\n",
    "tuner_args[\"objective_foo\"] = objective_rf\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_rf\"\n",
    "tuner_rf = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_extra_trees)\n",
    "tuner_args[\"objective_foo\"] = objective_extra_trees\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_extra_trees\"\n",
    "tuner_extra_trees = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_logistic_reg)\n",
    "tuner_args[\"objective_foo\"] = objective_logistic_reg\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_logistic\"\n",
    "tuner_logistic_reg = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_svm)\n",
    "tuner_args[\"objective_foo\"] = objective_svm\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_svm\"\n",
    "tuner_svm = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_knn)\n",
    "tuner_args[\"objective_foo\"] = objective_knn\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_knn\"\n",
    "tuner_knn = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_smbo)\n",
    "tuner_args[\"objective_foo\"] = objective_smbo\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_smbo\"\n",
    "tuner_smbo = TunerSMAC(**tuner_args)\n",
    "\n",
    "tuner_args[\"hp_dict\"] = deepcopy(hp_dict_mlp)\n",
    "tuner_args[\"objective_foo\"] = objective_mlp\n",
    "tuner_args[\"log_path\"] = base_dir + \"test_mlp\"\n",
    "tuner_mlp = TunerSMAC(**tuner_args)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SMBO\n",
    "tuner_smbo.tune(40*10)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
